{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import losses\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "import utils\n",
    "from model import SEDD\n",
    "from model.ema import ExponentialMovingAverage\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data.synthetic import utils as data_utils\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import functools\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path='configs//synthetic_config_masked.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//tiny.yaml')\n",
    "work_dir = 'for_synthetic_data_masked'\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices.\n",
      "NVIDIA GeForce GTX 1080 \t Memory: 8.00GB\n",
      "Number of parameters in the model: 609795\n",
      "SEDD(\n",
      "  (vocab_embed): EmbeddingLayer()\n",
      "  (sigma_map): TimestepEmbedder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rotary_emb): Rotary()\n",
      "  (blocks): ModuleList(\n",
      "    (0-2): 3 x DDiTBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (attn_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "      (attn_out): Linear(in_features=64, out_features=64, bias=False)\n",
      "      (dropout1): Dropout(p=0.01, inplace=False)\n",
      "      (norm2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='tanh')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.01, inplace=False)\n",
      "      (adaLN_modulation): Linear(in_features=256, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): DDitFinalLayer(\n",
      "    (norm_final): LayerNorm()\n",
      "    (linear): Linear(in_features=64, out_features=3, bias=True)\n",
      "    (adaLN_modulation): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      ")\n",
      "EMA: <model.ema.ExponentialMovingAverage object at 0x000002391687A850>\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 5e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scaler: <torch.cuda.amp.grad_scaler.GradScaler object at 0x0000023949817940>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_23368\\1171143107.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Create directories for experimental logs\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "utils.makedirs(sample_dir)\n",
    "utils.makedirs(checkpoint_dir)\n",
    "utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"WARNING: Using device {}\".format(device))\n",
    "    print(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "# build token graph\n",
    "graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "# build score model\n",
    "score_model = SEDD(cfg).to(device)\n",
    "#score_model = DDP(score_model, device_ids=[rank], static_graph=True, find_unused_parameters=True) Z:Commented this out\n",
    "\n",
    "num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")\n",
    "ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "print(score_model)\n",
    "print(f\"EMA: {ema}\")\n",
    "\n",
    "# build noise\n",
    "noise = noise_lib.get_noise(cfg).to(device)\n",
    "#noise = DDP(noise, device_ids=[rank], static_graph=True) Z:Commented this out\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "# build optimization state\n",
    "optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f\"Scaler: {scaler}\")\n",
    "state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n",
    "initial_step = int(state['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (10000000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from file - assumes we have already generated samples using generate_data.ipynb\n",
    "data_file = os.path.join(cfg.data.train, 'data.npy')\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = np.load(f).astype(np.int64)\n",
    "    print('data shape: %s' % str(data.shape))\n",
    "\n",
    "# Define a custom Dataset to wrap the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create a Dataset instance\n",
    "train_set = CustomDataset(data)\n",
    "\n",
    "# Function to cycle through DataLoader\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "# Initialize DataLoader without a sampler\n",
    "train_ds = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    batch_size=cfg.training.batch_size // (cfg.ngpus * cfg.training.accum),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,  # Shuffle the data as needed\n",
    "    persistent_workers=False,\n",
    "))\n",
    "\n",
    "# Create an iterator for the data\n",
    "train_iter = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n"
     ]
    }
   ],
   "source": [
    "#plotting code borrowed from Sun2023\n",
    "\n",
    "def plot(xbin, fn_xbin2float, output_file=None):\n",
    "  \"\"\"Visualize binary data.\"\"\"\n",
    "  float_data = fn_xbin2float(xbin)\n",
    "  if output_file is None:  # in-memory plot\n",
    "    buf = io.BytesIO()\n",
    "    data_utils.plot_samples(float_data, buf, im_size=4.1, im_fmt='png')\n",
    "    buf.seek(0)\n",
    "    image = np.asarray(PIL.Image.open(buf))[None, ...]\n",
    "    return image\n",
    "  else:\n",
    "    with open(output_file, 'wb') as f:\n",
    "      im_fmt = 'png' if output_file.endswith('.png') else 'pdf'\n",
    "      data_utils.plot_samples(float_data, f, im_size=4.1, im_fmt=im_fmt)\n",
    "\n",
    "\n",
    "class BinarySyntheticHelper(object):\n",
    "  \"\"\"Binary synthetic model helper.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.bm, self.inv_bm = data_utils.get_binmap(config.model.length,\n",
    "                                                 'gray')\n",
    "\n",
    "  def plot(self, xbin, output_file=None):\n",
    "    fn_xbin2float = functools.partial(\n",
    "        data_utils.bin2float, inv_bm=self.inv_bm,\n",
    "        discrete_dim=self.config.model.length, int_scale=self.config.int_scale)\n",
    "    return plot(xbin, fn_xbin2float, output_file)\n",
    "  \n",
    "model_helper = BinarySyntheticHelper(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metric used in Table 1 of Lou2023\n",
    "def binary_mmd(x, y, sim_fn):\n",
    "  \"\"\"MMD for binary data.\"\"\"\n",
    "  x = x.astype(np.float32)\n",
    "  y = y.astype(np.float32)\n",
    "  kxx = sim_fn(x, x)\n",
    "  kxx = kxx * (1 - np.eye(x.shape[0]))\n",
    "  kxx = np.sum(kxx) / x.shape[0] / (x.shape[0] - 1)\n",
    "\n",
    "  kyy = sim_fn(y, y)\n",
    "  kyy = kyy * (1 - np.eye(y.shape[0]))\n",
    "  kyy = np.sum(kyy) / y.shape[0] / (y.shape[0] - 1)\n",
    "  kxy = np.sum(sim_fn(x, y))\n",
    "  kxy = kxy / x.shape[0] / y.shape[0]\n",
    "  mmd = kxx + kyy - 2 * kxy\n",
    "  return mmd\n",
    "\n",
    "def binary_exp_hamming_sim(x, y, bd):\n",
    "  x = np.expand_dims(x, axis=1)\n",
    "  y = np.expand_dims(y, axis=0)\n",
    "  d = np.sum(np.abs(x - y), axis=-1)\n",
    "  return np.exp(-bd * d)\n",
    "\n",
    "def binary_exp_hamming_mmd(x, y, bandwidth=0.1):\n",
    "  sim_fn = functools.partial(binary_exp_hamming_sim, bd=bandwidth)\n",
    "  return binary_mmd(x, y, sim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at step 10001.\n"
     ]
    }
   ],
   "source": [
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(cfg)\n",
    "train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "\n",
    "if cfg.training.snapshot_sampling:\n",
    "        sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "        sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "\n",
    "\n",
    "num_train_steps = cfg.training.n_iters\n",
    "print(f\"Starting training loop at step {initial_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11000, training_loss: 2.19158e+01\n",
      "step: 12000, training_loss: 2.30325e+01\n",
      "step: 13000, training_loss: 2.24266e+01\n",
      "step: 14000, training_loss: 2.32441e+01\n",
      "step: 15000, training_loss: 2.16463e+01\n",
      "step: 16000, training_loss: 2.23951e+01\n",
      "step: 17000, training_loss: 2.20677e+01\n",
      "step: 18000, training_loss: 2.21815e+01\n",
      "step: 19000, training_loss: 2.21960e+01\n",
      "step: 20000, training_loss: 2.21214e+01\n",
      "Eval round 0 mmd: 0.007332335078250596\n",
      "Average mmd : 0.007332335078250596\n",
      "step: 21000, training_loss: 2.56982e+01\n",
      "step: 22000, training_loss: 2.22217e+01\n",
      "step: 23000, training_loss: 2.23996e+01\n",
      "step: 24000, training_loss: 2.26569e+01\n",
      "step: 25000, training_loss: 2.22019e+01\n",
      "step: 26000, training_loss: 2.33324e+01\n",
      "step: 27000, training_loss: 2.24384e+01\n",
      "step: 28000, training_loss: 2.24196e+01\n",
      "step: 29000, training_loss: 2.22064e+01\n",
      "step: 30000, training_loss: 2.20200e+01\n",
      "Eval round 0 mmd: 0.00706799882557424\n",
      "Average mmd : 0.00706799882557424\n",
      "step: 31000, training_loss: 2.22789e+01\n",
      "step: 32000, training_loss: 2.22671e+01\n",
      "step: 33000, training_loss: 2.18555e+01\n",
      "step: 34000, training_loss: 2.32355e+01\n",
      "step: 35000, training_loss: 2.23360e+01\n",
      "step: 36000, training_loss: 2.15696e+01\n",
      "step: 37000, training_loss: 2.13763e+01\n",
      "step: 38000, training_loss: 2.21377e+01\n",
      "step: 39000, training_loss: 2.46047e+01\n",
      "step: 40000, training_loss: 2.18168e+01\n",
      "Eval round 0 mmd: 0.009107040738174144\n",
      "Average mmd : 0.009107040738174144\n",
      "step: 41000, training_loss: 2.27338e+01\n",
      "step: 42000, training_loss: 2.22877e+01\n",
      "step: 43000, training_loss: 2.13745e+01\n",
      "step: 44000, training_loss: 2.16664e+01\n",
      "step: 45000, training_loss: 2.13148e+01\n",
      "step: 46000, training_loss: 2.43812e+01\n",
      "step: 47000, training_loss: 2.20637e+01\n",
      "step: 48000, training_loss: 2.24896e+01\n",
      "step: 49000, training_loss: 1.97088e+01\n",
      "step: 50000, training_loss: 2.15683e+01\n",
      "Eval round 0 mmd: 0.0075882015958446125\n",
      "Average mmd : 0.0075882015958446125\n",
      "step: 51000, training_loss: 2.13072e+01\n",
      "step: 52000, training_loss: 2.33289e+01\n",
      "step: 53000, training_loss: 2.20765e+01\n",
      "step: 54000, training_loss: 2.11160e+01\n",
      "step: 55000, training_loss: 2.16342e+01\n",
      "step: 56000, training_loss: 2.41995e+01\n",
      "step: 57000, training_loss: 2.24282e+01\n",
      "step: 58000, training_loss: 2.21566e+01\n",
      "step: 59000, training_loss: 2.25496e+01\n",
      "step: 60000, training_loss: 2.10121e+01\n",
      "Eval round 0 mmd: 0.006965601616119144\n",
      "Average mmd : 0.006965601616119144\n",
      "step: 61000, training_loss: 3.03517e+01\n",
      "step: 62000, training_loss: 2.16174e+01\n",
      "step: 63000, training_loss: 2.23321e+01\n",
      "step: 64000, training_loss: 2.13866e+01\n",
      "step: 65000, training_loss: 2.14407e+01\n",
      "step: 66000, training_loss: 2.26183e+01\n",
      "step: 67000, training_loss: 2.19550e+01\n",
      "step: 68000, training_loss: 2.26986e+01\n",
      "step: 69000, training_loss: 2.13652e+01\n",
      "step: 70000, training_loss: 2.33952e+01\n",
      "Eval round 0 mmd: 0.0069131017778587545\n",
      "Average mmd : 0.0069131017778587545\n",
      "step: 71000, training_loss: 2.16249e+01\n",
      "step: 72000, training_loss: 2.24266e+01\n",
      "step: 73000, training_loss: 2.16004e+01\n",
      "step: 74000, training_loss: 2.29016e+01\n",
      "step: 75000, training_loss: 2.36812e+01\n",
      "step: 76000, training_loss: 2.20906e+01\n",
      "step: 77000, training_loss: 2.21151e+01\n",
      "step: 78000, training_loss: 2.22292e+01\n",
      "step: 79000, training_loss: 2.24042e+01\n",
      "step: 80000, training_loss: 2.24134e+01\n",
      "Eval round 0 mmd: 0.006662536945500064\n",
      "Average mmd : 0.006662536945500064\n",
      "step: 81000, training_loss: 2.24616e+01\n",
      "step: 82000, training_loss: 2.25158e+01\n",
      "step: 83000, training_loss: 2.27632e+01\n",
      "step: 84000, training_loss: 2.12684e+01\n",
      "step: 85000, training_loss: 2.27020e+01\n",
      "step: 86000, training_loss: 2.22475e+01\n",
      "step: 87000, training_loss: 2.21272e+01\n",
      "step: 88000, training_loss: 2.28715e+01\n",
      "step: 89000, training_loss: 2.17851e+01\n",
      "step: 90000, training_loss: 2.11235e+01\n",
      "Eval round 0 mmd: 0.007585202739551267\n",
      "Average mmd : 0.007585202739551267\n",
      "step: 91000, training_loss: 2.17714e+01\n",
      "step: 92000, training_loss: 2.21544e+01\n",
      "step: 93000, training_loss: 2.17116e+01\n",
      "step: 94000, training_loss: 2.35148e+01\n",
      "step: 95000, training_loss: 2.07901e+01\n",
      "step: 96000, training_loss: 2.17730e+01\n",
      "step: 97000, training_loss: 2.22224e+01\n",
      "step: 98000, training_loss: 2.17948e+01\n",
      "step: 99000, training_loss: 2.13014e+01\n",
      "step: 100000, training_loss: 2.17986e+01\n",
      "Eval round 0 mmd: 0.0072730797704364525\n",
      "Average mmd : 0.0072730797704364525\n",
      "step: 101000, training_loss: 2.16444e+01\n",
      "step: 102000, training_loss: 2.21847e+01\n",
      "step: 103000, training_loss: 2.14917e+01\n",
      "step: 104000, training_loss: 2.44254e+01\n",
      "step: 105000, training_loss: 2.22755e+01\n",
      "step: 106000, training_loss: 2.09607e+01\n",
      "step: 107000, training_loss: 2.09343e+01\n",
      "step: 108000, training_loss: 2.22334e+01\n",
      "step: 109000, training_loss: 2.23043e+01\n",
      "step: 110000, training_loss: 2.16666e+01\n",
      "Eval round 0 mmd: 0.00718767554665356\n",
      "Average mmd : 0.00718767554665356\n",
      "step: 111000, training_loss: 2.22347e+01\n",
      "step: 112000, training_loss: 2.15198e+01\n",
      "step: 113000, training_loss: 2.17890e+01\n",
      "step: 114000, training_loss: 2.16214e+01\n",
      "step: 115000, training_loss: 2.15422e+01\n",
      "step: 116000, training_loss: 2.25191e+01\n",
      "step: 117000, training_loss: 2.15616e+01\n",
      "step: 118000, training_loss: 2.26059e+01\n",
      "step: 119000, training_loss: 2.19109e+01\n",
      "step: 120000, training_loss: 2.23251e+01\n",
      "Eval round 0 mmd: 0.007964139442060647\n",
      "Average mmd : 0.007964139442060647\n",
      "step: 121000, training_loss: 2.17281e+01\n",
      "step: 122000, training_loss: 2.24929e+01\n",
      "step: 123000, training_loss: 2.15756e+01\n",
      "step: 124000, training_loss: 2.19615e+01\n",
      "step: 125000, training_loss: 2.12866e+01\n",
      "step: 126000, training_loss: 2.00091e+01\n",
      "step: 127000, training_loss: 2.35184e+01\n",
      "step: 128000, training_loss: 2.24101e+01\n",
      "step: 129000, training_loss: 2.19716e+01\n",
      "step: 130000, training_loss: 2.37821e+01\n",
      "Eval round 0 mmd: 0.007200475237113535\n",
      "Average mmd : 0.007200475237113535\n",
      "step: 131000, training_loss: 2.23709e+01\n",
      "step: 132000, training_loss: 2.07962e+01\n",
      "step: 133000, training_loss: 2.22104e+01\n",
      "step: 134000, training_loss: 2.14414e+01\n",
      "step: 135000, training_loss: 2.10741e+01\n",
      "step: 136000, training_loss: 2.26797e+01\n",
      "step: 137000, training_loss: 2.16174e+01\n",
      "step: 138000, training_loss: 2.10370e+01\n",
      "step: 139000, training_loss: 2.26707e+01\n",
      "step: 140000, training_loss: 2.15786e+01\n",
      "Eval round 0 mmd: 0.0068166251932864075\n",
      "Average mmd : 0.0068166251932864075\n",
      "step: 141000, training_loss: 2.10817e+01\n",
      "step: 142000, training_loss: 2.23843e+01\n",
      "step: 143000, training_loss: 2.25950e+01\n",
      "step: 144000, training_loss: 2.14778e+01\n",
      "step: 145000, training_loss: 2.16653e+01\n",
      "step: 146000, training_loss: 2.26923e+01\n",
      "step: 147000, training_loss: 2.20532e+01\n",
      "step: 148000, training_loss: 2.24118e+01\n",
      "step: 149000, training_loss: 2.33425e+01\n",
      "step: 150000, training_loss: 2.15859e+01\n",
      "Eval round 0 mmd: 0.005696233679512663\n",
      "Average mmd : 0.005696233679512663\n",
      "step: 151000, training_loss: 2.04152e+01\n",
      "step: 152000, training_loss: 2.10041e+01\n",
      "step: 153000, training_loss: 2.19996e+01\n",
      "step: 154000, training_loss: 2.17222e+01\n",
      "step: 155000, training_loss: 2.22095e+01\n",
      "step: 156000, training_loss: 2.12103e+01\n",
      "step: 157000, training_loss: 2.27028e+01\n",
      "step: 158000, training_loss: 2.19498e+01\n",
      "step: 159000, training_loss: 2.25530e+01\n",
      "step: 160000, training_loss: 2.30589e+01\n",
      "Eval round 0 mmd: 0.006432867106261897\n",
      "Average mmd : 0.006432867106261897\n",
      "step: 161000, training_loss: 2.22558e+01\n",
      "step: 162000, training_loss: 2.19724e+01\n",
      "step: 163000, training_loss: 2.42421e+01\n",
      "step: 164000, training_loss: 2.21098e+01\n",
      "step: 165000, training_loss: 2.19824e+01\n",
      "step: 166000, training_loss: 2.18495e+01\n",
      "step: 167000, training_loss: 2.26268e+01\n",
      "step: 168000, training_loss: 2.26399e+01\n",
      "step: 169000, training_loss: 2.12882e+01\n",
      "step: 170000, training_loss: 2.16103e+01\n",
      "Eval round 0 mmd: 0.006413042693924509\n",
      "Average mmd : 0.006413042693924509\n",
      "step: 171000, training_loss: 2.12906e+01\n",
      "step: 172000, training_loss: 2.22843e+01\n",
      "step: 173000, training_loss: 2.18429e+01\n",
      "step: 174000, training_loss: 2.16868e+01\n",
      "step: 175000, training_loss: 2.11625e+01\n",
      "step: 176000, training_loss: 2.21068e+01\n",
      "step: 177000, training_loss: 2.18007e+01\n",
      "step: 178000, training_loss: 2.19111e+01\n",
      "step: 179000, training_loss: 2.50438e+01\n",
      "step: 180000, training_loss: 2.18484e+01\n",
      "Eval round 0 mmd: 0.00686309208781305\n",
      "Average mmd : 0.00686309208781305\n",
      "step: 181000, training_loss: 2.12158e+01\n",
      "step: 182000, training_loss: 2.28715e+01\n",
      "step: 183000, training_loss: 2.17791e+01\n",
      "step: 184000, training_loss: 2.19586e+01\n",
      "step: 185000, training_loss: 2.18518e+01\n",
      "step: 186000, training_loss: 2.17704e+01\n",
      "step: 187000, training_loss: 2.28265e+01\n",
      "step: 188000, training_loss: 2.31945e+01\n",
      "step: 189000, training_loss: 2.32588e+01\n",
      "step: 190000, training_loss: 2.24681e+01\n",
      "Eval round 0 mmd: 0.007915115444072585\n",
      "Average mmd : 0.007915115444072585\n",
      "step: 191000, training_loss: 2.18737e+01\n",
      "step: 192000, training_loss: 2.32463e+01\n",
      "step: 193000, training_loss: 2.14515e+01\n",
      "step: 194000, training_loss: 2.01569e+01\n",
      "step: 195000, training_loss: 2.38206e+01\n",
      "step: 196000, training_loss: 2.27620e+01\n",
      "step: 197000, training_loss: 2.09686e+01\n",
      "step: 198000, training_loss: 2.15768e+01\n",
      "step: 199000, training_loss: 2.20739e+01\n",
      "step: 200000, training_loss: 2.28736e+01\n",
      "Eval round 0 mmd: 0.0063111140807556865\n",
      "Average mmd : 0.0063111140807556865\n",
      "step: 201000, training_loss: 2.18014e+01\n",
      "step: 202000, training_loss: 2.15468e+01\n",
      "step: 203000, training_loss: 2.24251e+01\n",
      "step: 204000, training_loss: 2.12326e+01\n",
      "step: 205000, training_loss: 2.17831e+01\n",
      "step: 206000, training_loss: 2.10729e+01\n",
      "step: 207000, training_loss: 2.26583e+01\n",
      "step: 208000, training_loss: 2.19566e+01\n",
      "step: 209000, training_loss: 2.26027e+01\n",
      "step: 210000, training_loss: 2.19068e+01\n",
      "Eval round 0 mmd: 0.0065389024628170755\n",
      "Average mmd : 0.0065389024628170755\n",
      "step: 211000, training_loss: 2.24504e+01\n",
      "step: 212000, training_loss: 2.26124e+01\n",
      "step: 213000, training_loss: 2.17469e+01\n",
      "step: 214000, training_loss: 2.15616e+01\n",
      "step: 215000, training_loss: 2.15616e+01\n",
      "step: 216000, training_loss: 2.24447e+01\n",
      "step: 217000, training_loss: 2.16934e+01\n",
      "step: 218000, training_loss: 2.26202e+01\n",
      "step: 219000, training_loss: 2.18953e+01\n",
      "step: 220000, training_loss: 2.12519e+01\n",
      "Eval round 0 mmd: 0.006560831598383665\n",
      "Average mmd : 0.006560831598383665\n",
      "step: 221000, training_loss: 2.23648e+01\n",
      "step: 222000, training_loss: 2.28792e+01\n",
      "step: 223000, training_loss: 2.25566e+01\n",
      "step: 224000, training_loss: 2.24872e+01\n",
      "step: 225000, training_loss: 2.20077e+01\n",
      "step: 226000, training_loss: 2.20297e+01\n",
      "step: 227000, training_loss: 2.20815e+01\n",
      "step: 228000, training_loss: 2.20030e+01\n",
      "step: 229000, training_loss: 2.15244e+01\n",
      "step: 230000, training_loss: 2.07023e+01\n",
      "Eval round 0 mmd: 0.006324899123387184\n",
      "Average mmd : 0.006324899123387184\n",
      "step: 231000, training_loss: 2.28178e+01\n",
      "step: 232000, training_loss: 2.22448e+01\n",
      "step: 233000, training_loss: 2.86052e+01\n",
      "step: 234000, training_loss: 2.19882e+01\n",
      "step: 235000, training_loss: 2.16905e+01\n",
      "step: 236000, training_loss: 2.16807e+01\n",
      "step: 237000, training_loss: 2.15837e+01\n",
      "step: 238000, training_loss: 2.19805e+01\n",
      "step: 239000, training_loss: 2.24106e+01\n",
      "step: 240000, training_loss: 2.07397e+01\n",
      "Eval round 0 mmd: 0.006360577268063161\n",
      "Average mmd : 0.006360577268063161\n",
      "step: 241000, training_loss: 2.15598e+01\n",
      "step: 242000, training_loss: 2.18289e+01\n",
      "step: 243000, training_loss: 2.19106e+01\n",
      "step: 244000, training_loss: 2.15194e+01\n",
      "step: 245000, training_loss: 2.30328e+01\n",
      "step: 246000, training_loss: 2.24182e+01\n",
      "step: 247000, training_loss: 2.26914e+01\n",
      "step: 248000, training_loss: 2.43789e+01\n",
      "step: 249000, training_loss: 2.37759e+01\n",
      "step: 250000, training_loss: 2.18030e+01\n",
      "Eval round 0 mmd: 0.007590260923158065\n",
      "Average mmd : 0.007590260923158065\n",
      "step: 251000, training_loss: 2.14097e+01\n",
      "step: 252000, training_loss: 2.29991e+01\n",
      "step: 253000, training_loss: 2.11730e+01\n",
      "step: 254000, training_loss: 2.24953e+01\n",
      "step: 255000, training_loss: 2.20558e+01\n",
      "step: 256000, training_loss: 2.35007e+01\n",
      "step: 257000, training_loss: 2.07767e+01\n",
      "step: 258000, training_loss: 2.02241e+01\n",
      "step: 259000, training_loss: 2.16478e+01\n",
      "step: 260000, training_loss: 2.34830e+01\n",
      "Eval round 0 mmd: 0.006550341177816388\n",
      "Average mmd : 0.006550341177816388\n",
      "step: 261000, training_loss: 2.20682e+01\n",
      "step: 262000, training_loss: 2.19481e+01\n",
      "step: 263000, training_loss: 2.02180e+01\n",
      "step: 264000, training_loss: 2.11312e+01\n",
      "step: 265000, training_loss: 2.18801e+01\n",
      "step: 266000, training_loss: 2.29501e+01\n",
      "step: 267000, training_loss: 2.24744e+01\n",
      "step: 268000, training_loss: 2.31320e+01\n",
      "step: 269000, training_loss: 2.24799e+01\n",
      "step: 270000, training_loss: 2.26054e+01\n",
      "Eval round 0 mmd: 0.007844042808959673\n",
      "Average mmd : 0.007844042808959673\n",
      "step: 271000, training_loss: 2.23071e+01\n",
      "step: 272000, training_loss: 2.30215e+01\n",
      "step: 273000, training_loss: 2.24978e+01\n",
      "step: 274000, training_loss: 2.29611e+01\n",
      "step: 275000, training_loss: 2.16550e+01\n",
      "step: 276000, training_loss: 2.26682e+01\n",
      "step: 277000, training_loss: 2.15972e+01\n",
      "step: 278000, training_loss: 2.26999e+01\n",
      "step: 279000, training_loss: 2.13482e+01\n",
      "step: 280000, training_loss: 2.15165e+01\n",
      "Eval round 0 mmd: 0.006848114439690134\n",
      "Average mmd : 0.006848114439690134\n",
      "step: 281000, training_loss: 2.22868e+01\n",
      "step: 282000, training_loss: 2.19544e+01\n",
      "step: 283000, training_loss: 2.35898e+01\n",
      "step: 284000, training_loss: 2.23412e+01\n",
      "step: 285000, training_loss: 2.21960e+01\n",
      "step: 286000, training_loss: 2.29342e+01\n",
      "step: 287000, training_loss: 2.16773e+01\n",
      "step: 288000, training_loss: 2.18360e+01\n",
      "step: 289000, training_loss: 2.42779e+01\n",
      "step: 290000, training_loss: 2.11731e+01\n",
      "Eval round 0 mmd: 0.007137813940190196\n",
      "Average mmd : 0.007137813940190196\n",
      "step: 291000, training_loss: 2.22338e+01\n",
      "step: 292000, training_loss: 2.19108e+01\n",
      "step: 293000, training_loss: 2.24377e+01\n",
      "step: 294000, training_loss: 2.15108e+01\n",
      "step: 295000, training_loss: 2.10425e+01\n",
      "step: 296000, training_loss: 2.24189e+01\n",
      "step: 297000, training_loss: 2.29424e+01\n",
      "step: 298000, training_loss: 2.17031e+01\n",
      "step: 299000, training_loss: 2.38426e+01\n",
      "step: 300000, training_loss: 2.25996e+01\n",
      "Eval round 0 mmd: 0.008002556930311766\n",
      "Average mmd : 0.008002556930311766\n"
     ]
    }
   ],
   "source": [
    "while state['step'] < num_train_steps + 1:\n",
    "    step = state['step']\n",
    "    batch=next(train_iter).to(device)\n",
    "    loss = train_step_fn(state, batch)\n",
    "\n",
    "    # flag to see if there was movement ie a full batch got computed\n",
    "    if step != state['step']:\n",
    "        if step % cfg.training.log_freq == 0:\n",
    "            print(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n",
    "            \n",
    "    if step % cfg.training.snapshot_freq_for_preemption == 0:\n",
    "        utils.save_checkpoint(checkpoint_meta_dir, state)\n",
    "\n",
    "    if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "        # Save the checkpoint.\n",
    "        save_step = step // cfg.training.snapshot_freq\n",
    "        utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "        \n",
    "        #want to use the ema weights for sampling\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print the metric used in Table1 of Lou2023. Should get at least as small as 1.62e-5 to be considered done training. \n",
    "        avg_mmd = 0.0\n",
    "        for i in range(cfg.eval_rounds):\n",
    "            gt_data = []\n",
    "            for _ in range(cfg.plot_samples // cfg.training.batch_size):\n",
    "                gt_data.append(next(train_ds).cpu().numpy())\n",
    "            gt_data = np.concatenate(gt_data, axis=0)\n",
    "            gt_data = np.reshape(gt_data, (-1, cfg.model.length))\n",
    "            sample_data=[]\n",
    "            for _ in range(cfg.plot_samples // cfg.training.batch_size):\n",
    "                sample_data.append(sampling_fn(score_model).cpu().numpy())\n",
    "            sample_data=np.concatenate(sample_data,axis=0)\n",
    "            x0 = np.reshape(sample_data, gt_data.shape)\n",
    "            mmd = binary_exp_hamming_mmd(x0, gt_data)\n",
    "            avg_mmd += mmd\n",
    "            print(f'Eval round {i} mmd: {mmd}')\n",
    "            model_helper.plot(x0,f'step_{save_step}_eval_round_{i}.pdf') #plot a sample from the model\n",
    "\n",
    "        avg_mmd = avg_mmd / cfg.eval_rounds\n",
    "        print(f'Average mmd : {avg_mmd}')\n",
    "        with open('output.txt', 'a') as file:\n",
    "            file.write(f'step: {save_step}, loss: {loss.item()}, MMD: {avg_mmd}\\n')\n",
    "\n",
    "        ema.restore(score_model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
