{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import losses\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "import utils\n",
    "from model import SEDD\n",
    "from model.ema import ExponentialMovingAverage\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data.synthetic import utils as data_utils\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import functools\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path='configs//synthetic_config.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//tiny.yaml')\n",
    "work_dir = 'for_synthetic_data'\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices.\n",
      "NVIDIA GeForce GTX 1080 \t Memory: 8.00GB\n",
      "Number of parameters in the model: 3076355\n",
      "SEDD(\n",
      "  (vocab_embed): EmbeddingLayer()\n",
      "  (sigma_map): TimestepEmbedder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rotary_emb): Rotary()\n",
      "  (blocks): ModuleList(\n",
      "    (0-2): 3 x DDiTBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (attn_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "      (attn_out): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): GELU(approximate='tanh')\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (adaLN_modulation): Linear(in_features=128, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): DDitFinalLayer(\n",
      "    (norm_final): LayerNorm()\n",
      "    (linear): Linear(in_features=256, out_features=3, bias=True)\n",
      "    (adaLN_modulation): Linear(in_features=128, out_features=512, bias=True)\n",
      "  )\n",
      ")\n",
      "EMA: <model.ema.ExponentialMovingAverage object at 0x000001A2DC63EDF0>\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "Scaler: <torch.cuda.amp.grad_scaler.GradScaler object at 0x000001A2DC654D60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_20632\\1171143107.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(optimizer\u001b[38;5;241m=\u001b[39moptimizer, scaler\u001b[38;5;241m=\u001b[39mscaler, model\u001b[38;5;241m=\u001b[39mscore_model, noise\u001b[38;5;241m=\u001b[39mnoise, ema\u001b[38;5;241m=\u001b[39mema, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m---> 49\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_meta_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m initial_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\utils.py:55\u001b[0m, in \u001b[0;36mrestore_checkpoint\u001b[1;34m(ckpt_dir, state, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     loaded_state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     57\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mload_state_dict(loaded_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# Create directories for experimental logs\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "utils.makedirs(sample_dir)\n",
    "utils.makedirs(checkpoint_dir)\n",
    "utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"WARNING: Using device {}\".format(device))\n",
    "    print(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "# build token graph\n",
    "graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "# build score model\n",
    "score_model = SEDD(cfg).to(device)\n",
    "#score_model = DDP(score_model, device_ids=[rank], static_graph=True, find_unused_parameters=True) Z:Commented this out\n",
    "\n",
    "num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")\n",
    "ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "print(score_model)\n",
    "print(f\"EMA: {ema}\")\n",
    "\n",
    "# build noise\n",
    "noise = noise_lib.get_noise(cfg).to(device)\n",
    "#noise = DDP(noise, device_ids=[rank], static_graph=True) Z:Commented this out\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "# build optimization state\n",
    "optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f\"Scaler: {scaler}\")\n",
    "state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n",
    "initial_step = int(state['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (10000000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from file - assumes we have already generated samples using generate_data.ipynb\n",
    "data_file = os.path.join(cfg.data.train, 'data.npy')\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = np.load(f).astype(np.int64)\n",
    "    print('data shape: %s' % str(data.shape))\n",
    "\n",
    "# Define a custom Dataset to wrap the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create a Dataset instance\n",
    "train_set = CustomDataset(data)\n",
    "\n",
    "# Function to cycle through DataLoader\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "# Initialize DataLoader without a sampler\n",
    "train_ds = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    batch_size=cfg.training.batch_size // (cfg.ngpus * cfg.training.accum),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,  # Shuffle the data as needed\n",
    "    persistent_workers=False,\n",
    "))\n",
    "\n",
    "# Create an iterator for the data\n",
    "train_iter = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n"
     ]
    }
   ],
   "source": [
    "#plotting code borrowed from Sun2023\n",
    "\n",
    "def plot(xbin, fn_xbin2float, output_file=None):\n",
    "  \"\"\"Visualize binary data.\"\"\"\n",
    "  float_data = fn_xbin2float(xbin)\n",
    "  if output_file is None:  # in-memory plot\n",
    "    buf = io.BytesIO()\n",
    "    data_utils.plot_samples(float_data, buf, im_size=4.1, im_fmt='png')\n",
    "    buf.seek(0)\n",
    "    image = np.asarray(PIL.Image.open(buf))[None, ...]\n",
    "    return image\n",
    "  else:\n",
    "    with open(output_file, 'wb') as f:\n",
    "      im_fmt = 'png' if output_file.endswith('.png') else 'pdf'\n",
    "      data_utils.plot_samples(float_data, f, im_size=4.1, im_fmt=im_fmt)\n",
    "\n",
    "\n",
    "class BinarySyntheticHelper(object):\n",
    "  \"\"\"Binary synthetic model helper.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    self.bm, self.inv_bm = data_utils.get_binmap(config.model.length,\n",
    "                                                 'gray')\n",
    "\n",
    "  def plot(self, xbin, output_file=None):\n",
    "    fn_xbin2float = functools.partial(\n",
    "        data_utils.bin2float, inv_bm=self.inv_bm,\n",
    "        discrete_dim=self.config.model.length, int_scale=self.config.int_scale)\n",
    "    return plot(xbin, fn_xbin2float, output_file)\n",
    "  \n",
    "model_helper = BinarySyntheticHelper(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metric used in Table 1 of Lou2023\n",
    "def binary_mmd(x, y, sim_fn):\n",
    "  \"\"\"MMD for binary data.\"\"\"\n",
    "  x = x.astype(np.float32)\n",
    "  y = y.astype(np.float32)\n",
    "  kxx = sim_fn(x, x)\n",
    "  kxx = kxx * (1 - np.eye(x.shape[0]))\n",
    "  kxx = np.sum(kxx) / x.shape[0] / (x.shape[0] - 1)\n",
    "\n",
    "  kyy = sim_fn(y, y)\n",
    "  kyy = kyy * (1 - np.eye(y.shape[0]))\n",
    "  kyy = np.sum(kyy) / y.shape[0] / (y.shape[0] - 1)\n",
    "  kxy = np.sum(sim_fn(x, y))\n",
    "  kxy = kxy / x.shape[0] / y.shape[0]\n",
    "  mmd = kxx + kyy - 2 * kxy\n",
    "  return mmd\n",
    "\n",
    "def binary_exp_hamming_sim(x, y, bd):\n",
    "  x = np.expand_dims(x, axis=1)\n",
    "  y = np.expand_dims(y, axis=0)\n",
    "  d = np.sum(np.abs(x - y), axis=-1)\n",
    "  return np.exp(-bd * d)\n",
    "\n",
    "def binary_exp_hamming_mmd(x, y, bandwidth=0.1):\n",
    "  sim_fn = functools.partial(binary_exp_hamming_sim, bd=bandwidth)\n",
    "  return binary_mmd(x, y, sim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at step 0.\n"
     ]
    }
   ],
   "source": [
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(cfg)\n",
    "train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "\n",
    "if cfg.training.snapshot_sampling:\n",
    "        sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "        sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "\n",
    "\n",
    "num_train_steps = cfg.training.n_iters\n",
    "print(f\"Starting training loop at step {initial_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:294: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training_loss: 2.18760e+01\n",
      "step: 50, training_loss: 2.23778e+01\n",
      "step: 100, training_loss: 2.26526e+01\n",
      "step: 150, training_loss: 2.18569e+01\n",
      "step: 200, training_loss: 2.27651e+01\n",
      "step: 250, training_loss: 2.32102e+01\n",
      "step: 300, training_loss: 2.12762e+01\n",
      "step: 350, training_loss: 2.12275e+01\n",
      "step: 400, training_loss: 2.26328e+01\n",
      "step: 450, training_loss: 2.15490e+01\n",
      "step: 500, training_loss: 2.23804e+01\n",
      "step: 550, training_loss: 2.17534e+01\n",
      "step: 600, training_loss: 2.10654e+01\n",
      "step: 650, training_loss: 2.18728e+01\n",
      "step: 700, training_loss: 2.24838e+01\n",
      "step: 750, training_loss: 2.33135e+01\n",
      "step: 800, training_loss: 2.27365e+01\n",
      "step: 850, training_loss: 2.21174e+01\n",
      "step: 900, training_loss: 2.22074e+01\n",
      "step: 950, training_loss: 2.15072e+01\n",
      "step: 1000, training_loss: 2.28064e+01\n",
      "step: 1050, training_loss: 2.23140e+01\n",
      "step: 1100, training_loss: 2.18446e+01\n",
      "step: 1150, training_loss: 2.27880e+01\n",
      "step: 1200, training_loss: 2.33780e+01\n",
      "step: 1250, training_loss: 2.21000e+01\n",
      "step: 1300, training_loss: 2.32273e+01\n",
      "step: 1350, training_loss: 2.19375e+01\n",
      "step: 1400, training_loss: 2.29827e+01\n",
      "step: 1450, training_loss: 2.21509e+01\n",
      "step: 1500, training_loss: 2.21277e+01\n",
      "step: 1550, training_loss: 2.17820e+01\n",
      "step: 1600, training_loss: 2.24376e+01\n",
      "step: 1650, training_loss: 2.14373e+01\n",
      "step: 1700, training_loss: 2.32232e+01\n",
      "step: 1750, training_loss: 2.18845e+01\n",
      "step: 1800, training_loss: 2.29122e+01\n",
      "step: 1850, training_loss: 2.10629e+01\n",
      "step: 1900, training_loss: 2.23457e+01\n",
      "step: 1950, training_loss: 2.26969e+01\n",
      "step: 2000, training_loss: 2.23282e+01\n",
      "step: 2050, training_loss: 2.27621e+01\n",
      "step: 2100, training_loss: 2.31030e+01\n",
      "step: 2150, training_loss: 2.12224e+01\n",
      "step: 2200, training_loss: 2.18571e+01\n",
      "step: 2250, training_loss: 2.26073e+01\n",
      "step: 2300, training_loss: 2.39418e+01\n",
      "step: 2350, training_loss: 2.21426e+01\n",
      "step: 2400, training_loss: 2.08294e+01\n",
      "step: 2450, training_loss: 2.13518e+01\n",
      "step: 2500, training_loss: 2.26139e+01\n",
      "step: 2550, training_loss: 2.26119e+01\n",
      "step: 2600, training_loss: 2.17236e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m step \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(train_iter)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# flag to see if there was movement ie a full batch got computed\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m!=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\losses.py:92\u001b[0m, in \u001b[0;36mget_step_fn.<locals>.step_fn\u001b[1;34m(state, batch, cond)\u001b[0m\n\u001b[0;32m     90\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     91\u001b[0m scaler \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 92\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m/\u001b[39m accum\n\u001b[0;32m     94\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     96\u001b[0m accum_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\losses.py:28\u001b[0m, in \u001b[0;36mget_loss_fn.<locals>.loss_fn\u001b[1;34m(model, batch, cond, t, perturbed_batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m     perturbed_batch \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39msample_transition(batch, sigma[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m     27\u001b[0m log_score_fn \u001b[38;5;241m=\u001b[39m mutils\u001b[38;5;241m.\u001b[39mget_score_fn(model, train\u001b[38;5;241m=\u001b[39mtrain, sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m log_score \u001b[38;5;241m=\u001b[39m \u001b[43mlog_score_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mscore_entropy(log_score, sigma[:, \u001b[38;5;28;01mNone\u001b[39;00m], perturbed_batch, batch)\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m (dsigma[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m loss)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:47\u001b[0m, in \u001b[0;36mget_score_fn.<locals>.score_fn\u001b[1;34m(x, sigma)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore_fn\u001b[39m(x, sigma):\n\u001b[0;32m     46\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sampling:\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;66;03m# when sampling return true score (not log used for training)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m score\u001b[38;5;241m.\u001b[39mexp()\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:34\u001b[0m, in \u001b[0;36mget_model_fn.<locals>.model_fn\u001b[1;34m(x, sigma)\u001b[0m\n\u001b[0;32m     31\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# otherwise output the raw values (we handle mlm training in losses.py)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:298\u001b[0m, in \u001b[0;36mSEDD.forward\u001b[1;34m(self, indices, sigma)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)):\n\u001b[0;32m    296\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[i](x, rotary_cos_sin, c, seqlens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 298\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x, c)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_by_sigma:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsorb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt configured this to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:47\u001b[0m, in \u001b[0;36mautocast.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\torch\\amp\\autocast_mode.py:390\u001b[0m, in \u001b[0;36mautocast.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Drop the cache when we exit to a nesting level that's outside any instance of autocast.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast_decrement_nesting() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 390\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear_autocast_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n\u001b[0;32m    392\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_fastdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while state['step'] < num_train_steps + 1:\n",
    "    step = state['step']\n",
    "    batch=next(train_iter).to(device)\n",
    "    loss = train_step_fn(state, batch)\n",
    "\n",
    "    # flag to see if there was movement ie a full batch got computed\n",
    "    if step != state['step']:\n",
    "        if step % cfg.training.log_freq == 0:\n",
    "            print(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n",
    "            \n",
    "    if step % cfg.training.snapshot_freq_for_preemption == 0:\n",
    "        utils.save_checkpoint(checkpoint_meta_dir, state)\n",
    "\n",
    "    if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "        # Save the checkpoint.\n",
    "        save_step = step // cfg.training.snapshot_freq\n",
    "        utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "        \n",
    "        #want to use the ema weights for sampling\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print the metric used in Table1 of Lou2023. Should get at least as small as 1.62e-5 to be considered done training. \n",
    "        avg_mmd = 0.0\n",
    "        for i in range(cfg.eval_rounds):\n",
    "            gt_data = []\n",
    "            for _ in range(cfg.plot_samples // cfg.training.batch_size):\n",
    "                gt_data.append(next(train_ds).cpu().numpy())\n",
    "            gt_data = np.concatenate(gt_data, axis=0)\n",
    "            gt_data = np.reshape(gt_data, (-1, cfg.model.length))\n",
    "            sample_data=[]\n",
    "            for _ in range(cfg.plot_samples // cfg.training.batch_size):\n",
    "                sample_data.append(sampling_fn(score_model).cpu().numpy())\n",
    "            sample_data=np.concatenate(sample_data,axis=0)\n",
    "            x0 = np.reshape(sample_data, gt_data.shape)\n",
    "            mmd = binary_exp_hamming_mmd(x0, gt_data)\n",
    "            avg_mmd += mmd\n",
    "            print(f'Eval round {i} mmd: {mmd}')\n",
    "            model_helper.plot(x0,f'step_{step}_eval_round_{i}.pdf') #plot a sample from the model\n",
    "\n",
    "        avg_mmd = avg_mmd / cfg.eval_rounds\n",
    "        print(f'Average mmd : {avg_mmd}')\n",
    "\n",
    "        ema.restore(score_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:294: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "c:\\Users\\bezem\\Documents\\REU code\\model\\transformer.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "test=sampling_fn(score_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
