{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import rotary\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "#rotary.Rotary(config.model.hidden_size // config.model.n_heads)\n",
    "hidden_size=4\n",
    "n_heads=2\n",
    "seq_len=3\n",
    "batch_size=1\n",
    "rotary_emb=rotary.Rotary(hidden_size // n_heads)\n",
    "x=torch.rand([batch_size,seq_len,hidden_size]) #batch of size 1, sequence length 3, embedded into the hidden size 4. Embedding is all .1\n",
    "rotary_cos_sin = rotary_emb(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "         [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "         [0.4556, 0.6323, 0.3489, 0.4017]]])\n",
      "tensor([[[[[ 1.0000,  1.0000]],\n",
      "\n",
      "          [[ 1.0000,  1.0000]],\n",
      "\n",
      "          [[ 1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "         [[[ 0.5403,  0.5403]],\n",
      "\n",
      "          [[ 0.5403,  0.5403]],\n",
      "\n",
      "          [[ 1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "         [[[-0.4161, -0.4161]],\n",
      "\n",
      "          [[-0.4161, -0.4161]],\n",
      "\n",
      "          [[ 1.0000,  1.0000]]]]])\n",
      "tensor([[[[[0.0000, 0.0000]],\n",
      "\n",
      "          [[0.0000, 0.0000]],\n",
      "\n",
      "          [[0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.8415, 0.8415]],\n",
      "\n",
      "          [[0.8415, 0.8415]],\n",
      "\n",
      "          [[0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.9093, 0.9093]],\n",
      "\n",
      "          [[0.9093, 0.9093]],\n",
      "\n",
      "          [[0.0000, 0.0000]]]]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(rotary_cos_sin[0])\n",
    "print(rotary_cos_sin[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = torch.rand([batch_size,seq_len,3*hidden_size]) \n",
    "qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0223, 0.1689],\n",
      "          [0.2939, 0.5185]],\n",
      "\n",
      "         [[0.4194, 0.5529],\n",
      "          [0.9527, 0.0362]],\n",
      "\n",
      "         [[0.2081, 0.9298],\n",
      "          [0.7231, 0.7423]]]])\n",
      "tensor([[[[0.6977, 0.8000],\n",
      "          [0.1610, 0.2823]],\n",
      "\n",
      "         [[0.1852, 0.3734],\n",
      "          [0.3051, 0.9320]],\n",
      "\n",
      "         [[0.5263, 0.2437],\n",
      "          [0.5846, 0.0332]]]])\n",
      "tensor([[[[0.6816, 0.9152],\n",
      "          [0.3971, 0.8742]],\n",
      "\n",
      "         [[0.1759, 0.2698],\n",
      "          [0.1507, 0.0317]],\n",
      "\n",
      "         [[0.1387, 0.2422],\n",
      "          [0.8155, 0.7932]]]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(qkv[:, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_8316\\1714280401.py:1: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast(enabled=False):\n",
    "            cos, sin = rotary_cos_sin\n",
    "            qkv = rotary.apply_rotary_pos_emb(\n",
    "                qkv, cos.to(qkv.dtype), sin.to(qkv.dtype)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0223,  0.1689],\n",
       "          [ 0.2939,  0.5185]],\n",
       "\n",
       "         [[-0.2386,  0.6517],\n",
       "          [ 0.4843,  0.8212]],\n",
       "\n",
       "         [[-0.9321, -0.1977],\n",
       "          [-0.9759,  0.3486]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6977,  0.8000],\n",
       "          [ 0.1610,  0.2823]],\n",
       "\n",
       "         [[-0.2141,  0.3576],\n",
       "          [-0.6194,  0.7603]],\n",
       "\n",
       "         [[-0.4406,  0.3772],\n",
       "          [-0.2734,  0.5178]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6816, 0.9152],\n",
       "          [0.3971, 0.8742]],\n",
       "\n",
       "         [[0.1759, 0.2698],\n",
       "          [0.1507, 0.0317]],\n",
       "\n",
       "         [[0.1387, 0.2422],\n",
       "          [0.8155, 0.7932]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.scaled_dot_product_attention(\n",
    "                q, k, v,attn_mask=None\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(x, 'b s h d -> b s (h d)', b=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5444, 0.8954, 0.5606, 0.8977],\n",
       "         [0.1617, 0.1357, 0.1627, 0.1451],\n",
       "         [0.4552, 0.4998, 0.4634, 0.5066]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to try doing the same thing, but instead of <br>\n",
    "\n",
    "qkv = rotary.apply_rotary_pos_emb(\n",
    "                qkv, cos.to(qkv.dtype), sin.to(qkv.dtype)\n",
    "            )<br>\n",
    "do<br>\n",
    "<br>\n",
    "import flash_attn.layers.rotary<br>\n",
    "cos.to(qkv.dtype)<br>\n",
    "sin.to(qkv.dtype)<br>\n",
    "cos = cos[0,:,0,0,:cos.shape[-1]//2]<br>\n",
    "sin = sin[0,:,0,0,:sin.shape[-1]//2]<br>\n",
    "flash_attn.layers.rotary.apply_rotary_emb_qkv_(qkv, cos, sin)<br>\n",
    "<br>\n",
    "If after this qkv is different, then the issue is with their rotary embedding implementation.<br>\n",
    "<br>\n",
    "We also need to instead of <br>\n",
    "<br>\n",
    "x = F.scaled_dot_product_attention(<br>\n",
    "                q, k, v,attn_mask=None<br>\n",
    "            )<br>\n",
    "do<br>\n",
    "<br>\n",
    "qkv = rearrange(qkv, 'b s ... -> (b s) ...')<br>\n",
    "cu_seqlens = torch.arange(<br>\n",
    "                0, (batch_size + 1) * seq_len, step=seq_len,<br>\n",
    "                dtype=torch.int32, device=qkv.device)<br>\n",
    "from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func<br>\n",
    "x = flash_attn_varlen_qkvpacked_func(<br>\n",
    "            qkv, cu_seqlens, seq_len, 0., causal=False)<br>\n",
    "<br>\n",
    "if x is different then the issue is my adaptation of flash attn to standard attn<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
