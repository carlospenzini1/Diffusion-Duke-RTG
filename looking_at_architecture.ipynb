{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import SEDD\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path='configs//config.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//small.yaml')\n",
    "score_model = SEDD(cfg).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotary_emb.inv_freq\n"
     ]
    }
   ],
   "source": [
    "param_names=[param[0] for param in score_model.named_parameters()] #see if they are naming all their parameters\n",
    "for k, v in score_model.state_dict().items():\n",
    "    if k not in param_names:\n",
    "        print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model.state_dict()['rotary_emb.inv_freq'].shape #this is the only model parameter that we can't access via looking at named_parmeters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_embed.embedding\n",
      "torch.Size([50258, 768])\n",
      "sigma_map.mlp.0.weight\n",
      "torch.Size([128, 256])\n",
      "sigma_map.mlp.0.bias\n",
      "torch.Size([128])\n",
      "sigma_map.mlp.2.weight\n",
      "torch.Size([128, 128])\n",
      "sigma_map.mlp.2.bias\n",
      "torch.Size([128])\n",
      "blocks.0.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.0.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.0.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.0.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.0.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.0.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.0.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.0.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.0.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.0.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.1.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.1.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.1.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.1.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.1.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.1.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.1.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.1.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.1.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.1.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.2.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.2.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.2.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.2.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.2.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.2.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.2.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.2.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.2.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.2.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.3.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.3.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.3.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.3.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.3.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.3.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.3.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.3.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.3.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.3.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.4.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.4.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.4.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.4.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.4.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.4.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.4.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.4.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.4.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.4.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.5.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.5.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.5.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.5.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.5.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.5.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.5.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.5.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.5.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.5.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.6.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.6.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.6.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.6.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.6.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.6.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.6.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.6.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.6.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.6.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.7.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.7.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.7.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.7.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.7.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.7.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.7.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.7.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.7.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.7.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.8.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.8.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.8.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.8.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.8.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.8.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.8.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.8.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.8.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.8.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.9.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.9.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.9.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.9.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.9.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.9.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.9.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.9.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.9.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.9.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.10.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.10.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.10.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.10.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.10.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.10.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.10.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.10.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.10.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.10.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "blocks.11.norm1.weight\n",
      "torch.Size([768])\n",
      "blocks.11.attn_qkv.weight\n",
      "torch.Size([2304, 768])\n",
      "blocks.11.attn_out.weight\n",
      "torch.Size([768, 768])\n",
      "blocks.11.norm2.weight\n",
      "torch.Size([768])\n",
      "blocks.11.mlp.0.weight\n",
      "torch.Size([3072, 768])\n",
      "blocks.11.mlp.0.bias\n",
      "torch.Size([3072])\n",
      "blocks.11.mlp.2.weight\n",
      "torch.Size([768, 3072])\n",
      "blocks.11.mlp.2.bias\n",
      "torch.Size([768])\n",
      "blocks.11.adaLN_modulation.weight\n",
      "torch.Size([4608, 128])\n",
      "blocks.11.adaLN_modulation.bias\n",
      "torch.Size([4608])\n",
      "output_layer.norm_final.weight\n",
      "torch.Size([768])\n",
      "output_layer.linear.weight\n",
      "torch.Size([50258, 768])\n",
      "output_layer.linear.bias\n",
      "torch.Size([50258])\n",
      "output_layer.adaLN_modulation.weight\n",
      "torch.Size([1536, 128])\n",
      "output_layer.adaLN_modulation.bias\n",
      "torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "for param in score_model.named_parameters():\n",
    "    print(param[0])\n",
    "    print(param[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEDD(\n",
      "  (vocab_embed): EmbeddingLayer()\n",
      "  (sigma_map): TimestepEmbedder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rotary_emb): Rotary()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x DDiTBlock(\n",
      "      (norm1): LayerNorm()\n",
      "      (attn_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "      (attn_out): Linear(in_features=768, out_features=768, bias=False)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (norm2): LayerNorm()\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='tanh')\n",
      "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (adaLN_modulation): Linear(in_features=128, out_features=4608, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): DDitFinalLayer(\n",
      "    (norm_final): LayerNorm()\n",
      "    (linear): Linear(in_features=768, out_features=50258, bias=True)\n",
      "    (adaLN_modulation): Linear(in_features=128, out_features=1536, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(score_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 12 DDit blocks (these are slightly modified transformer blocks from https://github.com/facebookresearch/DiT) should all be the same in terms of the form of the functions. Let's just make sure that is indeed the convention used for the named parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_param_names=[]\n",
    "for param in score_model.named_parameters():\n",
    "    if param[0].startswith('blocks.0'):\n",
    "        block_param_names.append(param[0].partition('blocks.0.')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['norm1.weight',\n",
       " 'attn_qkv.weight',\n",
       " 'attn_out.weight',\n",
       " 'norm2.weight',\n",
       " 'mlp.0.weight',\n",
       " 'mlp.0.bias',\n",
       " 'mlp.2.weight',\n",
       " 'mlp.2.bias',\n",
       " 'adaLN_modulation.weight',\n",
       " 'adaLN_modulation.bias']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_param_size_dict={key:[] for key in block_param_names}\n",
    "for param in score_model.named_parameters():\n",
    "    if param[0].startswith('blocks'):\n",
    "        matches_name=False\n",
    "        for ending in block_param_names:\n",
    "            if param[0].endswith(ending):\n",
    "                block_param_size_dict[ending].append(param[1].shape)\n",
    "                matches_name=True\n",
    "        if matches_name==False:\n",
    "            print(f'Warning: paramater {param[0]} has a mismatched naming convention')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "{torch.Size([768])}\n",
      "12\n",
      "{torch.Size([2304, 768])}\n",
      "12\n",
      "{torch.Size([768, 768])}\n",
      "12\n",
      "{torch.Size([768])}\n",
      "12\n",
      "{torch.Size([3072, 768])}\n",
      "12\n",
      "{torch.Size([3072])}\n",
      "12\n",
      "{torch.Size([768, 3072])}\n",
      "12\n",
      "{torch.Size([768])}\n",
      "12\n",
      "{torch.Size([4608, 128])}\n",
      "12\n",
      "{torch.Size([4608])}\n"
     ]
    }
   ],
   "source": [
    "for value in block_param_size_dict.values():\n",
    "    print(len(value))\n",
    "    print(set(value))\n",
    "    if len(set(value))!=1:\n",
    "        print('Warning: mismatched size!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay good, so indeed we only need to understand the form of a given block to understand the form of the entire model. To summarize, we need to understand the form of the vocab embedding, the sigma map for the timestep embedding, the rotary embedding, a given DDit block, and the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class SEDD, we see self.vocab_embed = EmbeddingLayer(config.model.hidden_size, vocab_size). \n",
    "\n",
    "We also see the first line in the method forward for SEDD is x = self.vocab_embed(indices), so this is the first operation performed when passing a sequence into the network.\n",
    "\n",
    "The inputs into SEDD are indices and sigma, where I believe indices are the tokenized words (just represented by what position they take in the dictionary) and sigma is a time-step\n",
    "\n",
    "In config.yaml, we have vocab_size = 50257 and in small.yaml we have hidden_size = 768. \n",
    "\n",
    "For some reason they define their own embedding layer in transformer.py rather than using the standard nn.embedding. We see there that self.vocab_embed(indices) takes in a tensor of indices (necessarily natural numbers between 0 and vocab_size) and outputs for each index a vector of length 768.\n",
    "\n",
    "The matrix holding these 50257 vectors of length 768 is vocab_embed.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0110, -0.0273,  0.0203,  ...,  0.0276,  0.0107, -0.0220],\n",
       "         [-0.0201,  0.0239,  0.0255,  ...,  0.0160, -0.0254,  0.0331],\n",
       "         [ 0.0352, -0.0040,  0.0310,  ...,  0.0106,  0.0093,  0.0100],\n",
       "         [-0.0292, -0.0103,  0.0148,  ..., -0.0120, -0.0008, -0.0164]],\n",
       "\n",
       "        [[ 0.0097, -0.0285, -0.0270,  ...,  0.0166, -0.0031,  0.0328],\n",
       "         [ 0.0329,  0.0052,  0.0140,  ..., -0.0037,  0.0115,  0.0331],\n",
       "         [ 0.0172, -0.0130,  0.0213,  ...,  0.0102,  0.0156, -0.0292],\n",
       "         [-0.0351, -0.0051, -0.0329,  ..., -0.0021,  0.0172, -0.0065]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example on a batch of 2 sequences with length 4\n",
    "vocab_size=50257\n",
    "hidden_size=768\n",
    "embedding = nn.Parameter(torch.empty((vocab_size,hidden_size)))\n",
    "torch.nn.init.kaiming_uniform_(embedding, a=math.sqrt(5))\n",
    "indices=torch.tensor([[1,2,3,199],[5,6,7,40]])\n",
    "embedding[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have c = F.silu(self.sigma_map(sigma)). c is passed as an argument into each block and into the output layer. self.sigma_map = TimestepEmbedder(config.model.cond_dim).\n",
    "\n",
    "In small.yaml we see cond_dim=128. \n",
    "\n",
    "Looking at the forward method for TimestepEmbedder, we have \n",
    "def forward(self, t):\n",
    "    t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "    t_emb = self.mlp(t_freq)\n",
    "    return t_emb\n",
    "\n",
    "frequency_embedding_size has default 256, and timestep_embedding is some fixed fourier transform thing which is not learned during training. It takes in a tensor of times and returns for each a 256 dimensional vector.\n",
    "\n",
    "mlp is given by \n",
    "self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, cond_dim, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(cond_dim, cond_dim, bias=True),\n",
    "        )\n",
    "Looking at the torch documentation, https://pytorch.org/docs/stable/nn.html, we see that the 256 dimensional vector v from each time is transformed via W_2*silu(W_1v+b_1)+b_2, where W_1 is a cond_dim by 256 matrix, b_1 and b_2 cond_dim dimensional vectors, W_2 a cond_dim by cond_dim matrix, and silu is the element-wise silu function.\n",
    "\n",
    "These are sigma_map.mlp.0.weight, sigma_map.mlp.0.bias, sigma_map.mlp.2.weight, and sigma_map.mlp.2.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have rotary_cos_sin = self.rotary_emb(x). Supposing the input was a single sequence of L words, x will be L by hidden_size, since each word gets embedded into a hidden_size-dimensional vector.\n",
    "\n",
    "self.rotary_emb = rotary.Rotary(config.model.hidden_size // config.model.n_heads)\n",
    "\n",
    "In small.yaml, we have hidden_size = 768, n_heads=12.\n",
    "\n",
    "Looking at the Rotary class, we see inv_freq's values are not actually changed during training. This is also doing some kind of fourier thing, and returns two tensors of size...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 3, 1, 64])\n",
      "torch.Size([1, 768, 3, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "from model import rotary\n",
    "Rotary=rotary.Rotary(768/12)\n",
    "x=embedding[torch.tensor([1,2,3,4,5])]\n",
    "print(Rotary(x)[0].shape)\n",
    "print(Rotary(x)[1].shape)\n",
    "#okay I'm not sure what the role of this is tbh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have \n",
    "for i in range(len(self.blocks)):\n",
    "    x = self.blocks[i](x, rotary_cos_sin, c, seqlens=None)\n",
    "where \n",
    "self.blocks = nn.ModuleList([\n",
    "            DDiTBlock(config.model.hidden_size, config.model.n_heads, config.model.cond_dim, dropout=config.model.dropout) for _ in range(config.model.n_blocks)\n",
    "        ])\n",
    "\n",
    "In small.yaml, we have hidden_size = 768, n_heads = 12, cond_dim = 128, dropout= 0.1\n",
    "\n",
    "In forward for DDiTBlock:\n",
    "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
    "\n",
    "        bias_dropout_scale_fn = self._get_bias_dropout_scale()\n",
    "\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c)[:, None].chunk(6, dim=2)\n",
    "\n",
    "        # attention operation\n",
    "        x_skip = x\n",
    "        x = modulate_fused(self.norm1(x), shift_msa, scale_msa)\n",
    "        # dtype0 = x.dtype\n",
    "\n",
    "        qkv = self.attn_qkv(x)\n",
    "        qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=self.n_heads)\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            cos, sin = rotary_cos_sin\n",
    "            qkv = rotary.apply_rotary_pos_emb(\n",
    "                qkv, cos.to(qkv.dtype), sin.to(qkv.dtype)\n",
    "            )\n",
    "        \n",
    "        #qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n",
    "        #if seqlens is None:\n",
    "        #    cu_seqlens = torch.arange(\n",
    "        #        0, (batch_size + 1) * seq_len, step=seq_len,\n",
    "        #        dtype=torch.int32, device=qkv.device\n",
    "        #    )\n",
    "        #else:\n",
    "        #    cu_seqlens = seqlens.cumsum(-1)\n",
    "\n",
    "        #x = flash_attn_varlen_qkvpacked_func(\n",
    "        #    qkv, cu_seqlens, seq_len, 0., causal=False)\n",
    "        \n",
    "        #x = rearrange(x, '(b s) h d -> b s (h d)', b=batch_size)\n",
    "\n",
    "        # Z: I replaced the above with the below because I think flash attention was the thing which is picky about what GPU you use \n",
    "        \n",
    "        # Separate Q, K, V (b, s, h, d_head)\n",
    "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
    "\n",
    "\n",
    "        # Create Attention Mask for Variable-Length Sequences\n",
    "        if seqlens is not None:\n",
    "            attn_mask = torch.ones(batch_size, seq_len, seq_len, device=x.device) * float('-inf')\n",
    "            for i in range(batch_size):\n",
    "                valid_len = seqlens[i]\n",
    "                attn_mask[i, :valid_len, :valid_len] = 0  # Allow valid positions only\n",
    "        else:\n",
    "            attn_mask = None #not sure about this\n",
    "\n",
    "        # Apply Attention\n",
    "        x = F.scaled_dot_product_attention(\n",
    "                q, k, v,attn_mask=attn_mask\n",
    "            )\n",
    "        \n",
    "        x = rearrange(x, '(b s) h d -> b s (h d)', b=batch_size)\n",
    "\n",
    "        x = bias_dropout_scale_fn(self.attn_out(x), None, gate_msa, x_skip, self.dropout)\n",
    "\n",
    "        # mlp operation\n",
    "        x = bias_dropout_scale_fn(self.mlp(modulate_fused(self.norm2(x), shift_mlp, scale_mlp)), None, gate_mlp, x, self.dropout)\n",
    "\n",
    "adaLN_modulation is a linear transformation from cond_dim to 6*hidden_size, with matrix and vector given by blocks.0.adaLN_modulation.weight and blocks.0.adaLN_modulation.bias\n",
    "\n",
    "norm1 = LayerNorm(hidden_size). This normalizes the each vector representing a token of length hidden_size to mean 0 and variance 1 then multiplies it by a diagonal matrix with diagonal given by blocks.0.norm1.weight\n",
    "\n",
    "modulate_fused is just shifting and scaling x\n",
    "\n",
    "attn_qkv is a linear layer with no bias from hidden_size to 3*hidden_size with matrix given by blocks.0.attn_qkv.weight \n",
    "\n",
    "apply_rotary_pos_emb is the same operation as before, doesn't require any learning\n",
    "\n",
    "The bit with attention mask is to set the values past the length of the sequence to -infty when doing F.scaled_dot_product_attention\n",
    "\n",
    "F.scaled_dot_product_attention is actually applying the attention mechanism with qkv. I'm not sure how the masking works so IDK for sure if it is set up correctly. This is just a function of q k and v, doesn't introduce any new paramaters\n",
    "\n",
    "attn_out is a linear layer with no bias from hidden_size to hidden_size\n",
    "\n",
    "bias_dropout_scale_fn is using F.dropout to randomly zero some values in the tensor \n",
    "\n",
    "norm2 is another LayerNorm(hidden_size)\n",
    "\n",
    "self.mlp is nn.Sequential(\n",
    "            nn.Linear(dim, mlp_ratio * hidden_size, bias=True),\n",
    "            nn.GELU(approximate=\"tanh\"),\n",
    "            nn.Linear(mlp_ratio * hidden_size, hidden_size, bias=True)\n",
    "        )\n",
    "\n",
    "mlp_ratio defaults to 4\n",
    "\n",
    "\n",
    "It is plain to see then where each of these paramaters comes in:\n",
    "\n",
    "blocks.0.norm1.weight\n",
    "\n",
    "torch.Size([768])\n",
    "\n",
    "blocks.0.attn_qkv.weight\n",
    "\n",
    "torch.Size([2304, 768])\n",
    "\n",
    "blocks.0.attn_out.weight\n",
    "\n",
    "torch.Size([768, 768])\n",
    "\n",
    "blocks.0.norm2.weight\n",
    "\n",
    "torch.Size([768])\n",
    "\n",
    "blocks.0.mlp.0.weight\n",
    "\n",
    "torch.Size([3072, 768])\n",
    "\n",
    "blocks.0.mlp.0.bias\n",
    "\n",
    "torch.Size([3072])\n",
    "\n",
    "blocks.0.mlp.2.weight\n",
    "\n",
    "torch.Size([768, 3072])\n",
    "\n",
    "blocks.0.mlp.2.bias\n",
    "\n",
    "torch.Size([768])\n",
    "\n",
    "blocks.0.adaLN_modulation.weight\n",
    "\n",
    "torch.Size([4608, 128])\n",
    "\n",
    "blocks.0.adaLN_modulation.bias\n",
    "\n",
    "torch.Size([4608])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing used in foward for SEDD is \n",
    "\n",
    "x = self.output_layer(x, c)\n",
    "\n",
    "(other than some scatter thing that I'm not sure what its doing yet)\n",
    "\n",
    "self.output_layer=DDitFinalLayer(config.model.hidden_size, vocab_size, config.model.cond_dim)\n",
    "\n",
    "these are hidden_size: 768, tokens: 50257, cond_dim: 128\n",
    "\n",
    "forward for DDitFinalLater is:\n",
    "shift, scale = self.adaLN_modulation(c)[:, None].chunk(2, dim=2)\n",
    "x = modulate_fused(self.norm_final(x), shift, scale)\n",
    "x = self.linear(x)\n",
    "\n",
    "adaLN_modulation is nn.Linear(cond_dim, 2 * hidden_size, bias=True)\n",
    "norm final is LayerNorm(hidden_size)\n",
    "self.linear = nn.Linear(hidden_size, vocab_size). By default bias is true\n",
    "\n",
    "So this final later converts the hidden_size vector for each token to a vocab_size vector.\n",
    "\n",
    "This final output vector is used for our transition probabilities. In particular, for every position we now have a vector of length the number of tokens. So sample what token to update that position to, we will exponentiate each element and normalize to a probability vector. The x'th element is then interpreted as approximating p_t(x) in the score function. Unfortunately in machine learning the output vectors before applying this \"softmax\" function are also called the \"scores\" which is a bit confusing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
