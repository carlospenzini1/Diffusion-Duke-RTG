{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data.synthetic import utils as data_utils\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import functools\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def transformer_timestep_embedding(timesteps, dim, device,max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings (like in transformer position encodings).\n",
    "    timesteps: (batch,) or (N,)\n",
    "    Returns: (batch, dim)\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(0, half, dtype=torch.float32,device=device) / half)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:  # zero pad if needed\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class CatMLPScoreFunc(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len,cat_embed_size, num_layers, hidden_size, time_scale_factor=1000.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cat_embed_size = cat_embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_scale_factor = time_scale_factor\n",
    "        input_dim=cat_embed_size*seq_len\n",
    "        self.input_dim=input_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, cat_embed_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim,hidden_size))  # will init in forward\n",
    "        for _ in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(hidden_size,hidden_size))  # will init in forward\n",
    "        self.final = nn.Linear(hidden_size, seq_len*vocab_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) – categorical token ids\n",
    "        t: (batch,) – timesteps\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        V=self.vocab_size\n",
    "        x = self.embed(x)  # (B, L, cat_embed_size)\n",
    "        x = x.view(B, -1)  # (B, L * cat_embed_size)\n",
    "\n",
    "        temb = transformer_timestep_embedding(t * self.time_scale_factor, self.hidden_size,x.device).to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + temb\n",
    "            x = F.silu(x)\n",
    "\n",
    "        x = self.final(x)  # (B, L*vocab_size)\n",
    "        x=x.view(B, L, V)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_directory='data//synthetic//checkerboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices.\n",
      "NVIDIA GeForce GTX 1080 \t Memory: 8.00GB\n",
      "Number of parameters in the model: 2245952\n",
      "CatMLPScoreFunc(\n",
      "  (embed): Embedding(2, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (final): Linear(in_features=256, out_features=64, bias=True)\n",
      ")\n",
      "EMA: <model.ema.ExponentialMovingAverage object at 0x00000255D7AC3F10>\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-06\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "Scaler: <torch.cuda.amp.grad_scaler.GradScaler object at 0x00000255D7AC30A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_23024\\1715072068.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "2025-04-18 11:21:11,060 - No checkpoint found at for_synthetic_data_uniform\\checkpoints-meta\\checkpoint.pth. Returned the same state as input\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import utils\n",
    "from model.ema import ExponentialMovingAverage\n",
    "import losses\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "graph_type='mixed' #uniform, masked, or mixed\n",
    "\n",
    "\n",
    "cfg_path=f'configs//synthetic_config_{graph_type}.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//tiny.yaml')\n",
    "work_dir = f'for_synthetic_data_{graph_type}'\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "# Create directories for experimental logs\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "utils.makedirs(sample_dir)\n",
    "utils.makedirs(checkpoint_dir)\n",
    "utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"WARNING: Using device {}\".format(device))\n",
    "    print(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "# build token graph\n",
    "graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "# build score model\n",
    "batch_size = cfg.training.batch_size\n",
    "seq_len=32\n",
    "vocab_size = 2\n",
    "mask_id = vocab_size\n",
    "if graph_type=='uniform':\n",
    "    expand_vocab_size = vocab_size \n",
    "else:\n",
    "    expand_vocab_size = vocab_size + 1\n",
    "time_scale_factor=1000.0\n",
    "embed_dim=256\n",
    "num_layers=3\n",
    "\n",
    "score_model = CatMLPScoreFunc(vocab_size=expand_vocab_size,cat_embed_size=embed_dim,num_layers= num_layers,\n",
    "    hidden_size=embed_dim,seq_len=seq_len,time_scale_factor= 1000.0,\n",
    "    ).to(device)\n",
    "\n",
    "num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")\n",
    "ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "print(score_model)\n",
    "print(f\"EMA: {ema}\")\n",
    "\n",
    "# build noise\n",
    "noise = noise_lib.get_noise(cfg).to(device)\n",
    "#noise = DDP(noise, device_ids=[rank], static_graph=True) Z:Commented this out\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "# build optimization state\n",
    "optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f\"Scaler: {scaler}\")\n",
    "state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (10000000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from file - assumes we have already generated samples using generate_data.ipynb\n",
    "data_file = os.path.join(data_directory, 'data.npy')\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = np.load(f).astype(np.int64)\n",
    "    print('data shape: %s' % str(data.shape))\n",
    "\n",
    "# Define a custom Dataset to wrap the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create a Dataset instance\n",
    "train_set = CustomDataset(data)\n",
    "\n",
    "# Function to cycle through DataLoader\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "# Initialize DataLoader without a sampler\n",
    "train_ds = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,  # Shuffle the data as needed\n",
    "    persistent_workers=False,\n",
    "))\n",
    "\n",
    "# Create an iterator for the data\n",
    "train_iter = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n"
     ]
    }
   ],
   "source": [
    "#plotting code borrowed from Sun2023\n",
    "\n",
    "def plot(xbin, fn_xbin2float, output_file=None):\n",
    "  \"\"\"Visualize binary data.\"\"\"\n",
    "  float_data = fn_xbin2float(xbin)\n",
    "  if output_file is None:  # in-memory plot\n",
    "    buf = io.BytesIO()\n",
    "    data_utils.plot_samples(float_data, buf, im_size=4.1, im_fmt='png')\n",
    "    buf.seek(0)\n",
    "    image = np.asarray(PIL.Image.open(buf))[None, ...]\n",
    "    return image\n",
    "  else:\n",
    "    with open(output_file, 'wb') as f:\n",
    "      im_fmt = 'png' if output_file.endswith('.png') else 'pdf'\n",
    "      data_utils.plot_samples(float_data, f, im_size=4.1, im_fmt=im_fmt,axis=True)\n",
    "\n",
    "\n",
    "class BinarySyntheticHelper(object):\n",
    "  \"\"\"Binary synthetic model helper.\"\"\"\n",
    "\n",
    "  def __init__(self, seq_len,int_scale):\n",
    "    self.seq_len = seq_len\n",
    "    self.int_scale=int_scale\n",
    "    self.bm, self.inv_bm = data_utils.get_binmap(seq_len,\n",
    "                                                 'gray')\n",
    "\n",
    "  def plot(self, xbin, output_file=None):\n",
    "    fn_xbin2float = functools.partial(\n",
    "        data_utils.bin2float, inv_bm=self.inv_bm,\n",
    "        discrete_dim=self.seq_len, int_scale=self.int_scale)\n",
    "    return plot(xbin, fn_xbin2float, output_file)\n",
    "\n",
    "int_scale=5461.760975376213\n",
    "model_helper = BinarySyntheticHelper(seq_len,int_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metric used in Table 1 of Lou2023\n",
    "def binary_mmd(x, y, sim_fn):\n",
    "  \"\"\"MMD for binary data.\"\"\"\n",
    "  x = x.astype(np.float32)\n",
    "  y = y.astype(np.float32)\n",
    "  kxx = sim_fn(x, x)\n",
    "  kxx = kxx * (1 - np.eye(x.shape[0]))\n",
    "  kxx = np.sum(kxx) / x.shape[0] / (x.shape[0] - 1)\n",
    "\n",
    "  kyy = sim_fn(y, y)\n",
    "  kyy = kyy * (1 - np.eye(y.shape[0]))\n",
    "  kyy = np.sum(kyy) / y.shape[0] / (y.shape[0] - 1)\n",
    "  kxy = np.sum(sim_fn(x, y))\n",
    "  kxy = kxy / x.shape[0] / y.shape[0]\n",
    "  mmd = kxx + kyy - 2 * kxy\n",
    "  return mmd\n",
    "\n",
    "def binary_exp_hamming_sim(x, y, bd):\n",
    "  x = np.expand_dims(x, axis=1)\n",
    "  y = np.expand_dims(y, axis=0)\n",
    "  d = np.sum(np.abs(x - y), axis=-1)\n",
    "  return np.exp(-bd * d)\n",
    "\n",
    "def binary_exp_hamming_mmd(x, y, bandwidth=0.1):\n",
    "  sim_fn = functools.partial(binary_exp_hamming_sim, bd=bandwidth)\n",
    "  return binary_mmd(x, y, sim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at step 0.\n"
     ]
    }
   ],
   "source": [
    "num_train_steps=cfg.training.n_iters\n",
    "log_freq=cfg.training.log_freq\n",
    "snapshot_freq=cfg.training.snapshot_freq\n",
    "eval_rounds=1\n",
    "plot_samples=cfg.plot_samples\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(cfg)\n",
    "train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "num_train_steps = cfg.training.n_iters\n",
    "initial_step = int(state['step'])\n",
    "print(f\"Starting training loop at step {initial_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training_loss: 2.42984e+01\n",
      "step: 1000, training_loss: 2.35295e+01\n",
      "step: 2000, training_loss: 2.08440e+01\n",
      "step: 3000, training_loss: 1.99424e+01\n",
      "step: 4000, training_loss: 2.07609e+01\n",
      "step: 5000, training_loss: 3.32889e+01\n",
      "step: 6000, training_loss: 1.94826e+01\n",
      "step: 7000, training_loss: 1.96789e+01\n",
      "step: 8000, training_loss: 1.98326e+01\n",
      "step: 9000, training_loss: 2.04955e+01\n",
      "step: 10000, training_loss: 2.72859e+01\n",
      "Average mmd : 0.0001212035835974623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11000, training_loss: 2.67193e+01\n",
      "step: 12000, training_loss: 1.84116e+01\n",
      "step: 13000, training_loss: 2.13653e+01\n",
      "step: 14000, training_loss: 2.62131e+01\n",
      "step: 15000, training_loss: 2.14237e+01\n",
      "step: 16000, training_loss: 2.06137e+01\n",
      "step: 17000, training_loss: 1.48178e+01\n",
      "step: 18000, training_loss: 2.26144e+01\n",
      "step: 19000, training_loss: 2.12487e+01\n",
      "step: 20000, training_loss: 2.29754e+01\n",
      "Average mmd : 1.632814446689279e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21000, training_loss: 1.97591e+01\n",
      "step: 22000, training_loss: 1.81924e+01\n",
      "step: 23000, training_loss: 1.84218e+01\n",
      "step: 24000, training_loss: 2.00165e+01\n",
      "step: 25000, training_loss: 1.96742e+01\n",
      "step: 26000, training_loss: 1.84061e+01\n",
      "step: 27000, training_loss: 1.89762e+01\n",
      "step: 28000, training_loss: 1.77145e+01\n",
      "step: 29000, training_loss: 3.01783e+01\n",
      "step: 30000, training_loss: 2.55082e+01\n",
      "Average mmd : -5.340756874139263e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31000, training_loss: 2.04053e+01\n",
      "step: 32000, training_loss: 2.06433e+01\n",
      "step: 33000, training_loss: 1.89799e+01\n",
      "step: 34000, training_loss: 1.94992e+01\n",
      "step: 35000, training_loss: 2.11672e+01\n",
      "step: 36000, training_loss: 1.81417e+01\n",
      "step: 37000, training_loss: 2.31990e+01\n",
      "step: 38000, training_loss: 1.88216e+01\n",
      "step: 39000, training_loss: 2.11051e+01\n",
      "step: 40000, training_loss: 1.95474e+01\n",
      "Average mmd : 2.5428415665662563e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41000, training_loss: 1.83713e+01\n",
      "step: 42000, training_loss: 2.45525e+01\n",
      "step: 43000, training_loss: 2.84870e+01\n",
      "step: 44000, training_loss: 2.54536e+01\n",
      "step: 45000, training_loss: 2.30064e+01\n",
      "step: 46000, training_loss: 1.83735e+01\n",
      "step: 47000, training_loss: 1.92659e+01\n",
      "step: 48000, training_loss: 1.80435e+01\n",
      "step: 49000, training_loss: 1.95341e+01\n",
      "step: 50000, training_loss: 2.03498e+01\n",
      "Average mmd : 2.825103205378321e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51000, training_loss: 2.11859e+01\n",
      "step: 52000, training_loss: 1.85472e+01\n",
      "step: 53000, training_loss: 2.09118e+01\n",
      "step: 54000, training_loss: 2.56620e+01\n",
      "step: 55000, training_loss: 1.70449e+01\n",
      "step: 56000, training_loss: 2.08101e+01\n",
      "step: 57000, training_loss: 2.00293e+01\n",
      "step: 58000, training_loss: 3.78715e+01\n",
      "step: 59000, training_loss: 1.89577e+01\n",
      "step: 60000, training_loss: 1.68416e+01\n",
      "Average mmd : -5.002449323665559e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61000, training_loss: 2.10935e+01\n",
      "step: 62000, training_loss: 1.94929e+01\n",
      "step: 63000, training_loss: 1.72051e+01\n",
      "step: 64000, training_loss: 2.01241e+01\n",
      "step: 65000, training_loss: 1.91059e+01\n",
      "step: 66000, training_loss: 1.71747e+01\n",
      "step: 67000, training_loss: 1.71328e+01\n",
      "step: 68000, training_loss: 2.14887e+01\n",
      "step: 69000, training_loss: 1.93940e+01\n",
      "step: 70000, training_loss: 2.11639e+01\n",
      "Average mmd : -5.656282333710294e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 71000, training_loss: 1.93090e+01\n",
      "step: 72000, training_loss: 1.90613e+01\n",
      "step: 73000, training_loss: 3.55533e+01\n",
      "step: 74000, training_loss: 2.13146e+01\n",
      "step: 75000, training_loss: 2.36965e+01\n",
      "step: 76000, training_loss: 2.37611e+01\n",
      "step: 77000, training_loss: 1.74447e+01\n",
      "step: 78000, training_loss: 1.91886e+01\n",
      "step: 79000, training_loss: 1.81170e+01\n",
      "step: 80000, training_loss: 2.21272e+01\n",
      "Average mmd : 1.7922467292486033e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 81000, training_loss: 1.93499e+01\n",
      "step: 82000, training_loss: 1.79138e+01\n",
      "step: 83000, training_loss: 1.71285e+01\n",
      "step: 84000, training_loss: 2.06671e+01\n",
      "step: 85000, training_loss: 2.04101e+01\n",
      "step: 86000, training_loss: 2.42155e+01\n",
      "step: 87000, training_loss: 1.85299e+01\n",
      "step: 88000, training_loss: 1.68726e+01\n",
      "step: 89000, training_loss: 1.72409e+01\n",
      "step: 90000, training_loss: 1.81432e+01\n",
      "Average mmd : -3.711220933344528e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 91000, training_loss: 1.86171e+01\n",
      "step: 92000, training_loss: 1.87462e+01\n",
      "step: 93000, training_loss: 2.26295e+01\n",
      "step: 94000, training_loss: 2.15814e+01\n",
      "step: 95000, training_loss: 2.26632e+01\n",
      "step: 96000, training_loss: 1.95392e+01\n",
      "step: 97000, training_loss: 1.87940e+01\n",
      "step: 98000, training_loss: 1.69678e+01\n",
      "step: 99000, training_loss: 1.93619e+01\n",
      "step: 100000, training_loss: 1.75012e+01\n",
      "Average mmd : -1.0201209253379862e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 101000, training_loss: 2.12920e+01\n",
      "step: 102000, training_loss: 2.28504e+01\n",
      "step: 103000, training_loss: 1.79551e+01\n",
      "step: 104000, training_loss: 1.83885e+01\n",
      "step: 105000, training_loss: 2.64216e+01\n",
      "step: 106000, training_loss: 1.97800e+01\n",
      "step: 107000, training_loss: 1.95902e+01\n",
      "step: 108000, training_loss: 1.85535e+01\n",
      "step: 109000, training_loss: 1.78531e+01\n",
      "step: 110000, training_loss: 2.14032e+01\n",
      "Average mmd : -2.9705265058466157e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 111000, training_loss: 2.18645e+01\n",
      "step: 112000, training_loss: 1.82142e+01\n",
      "step: 113000, training_loss: 1.92611e+01\n",
      "step: 114000, training_loss: 3.28514e+01\n",
      "step: 115000, training_loss: 2.18657e+01\n",
      "step: 116000, training_loss: 2.02603e+01\n",
      "step: 117000, training_loss: 1.62913e+01\n",
      "step: 118000, training_loss: 1.77747e+01\n",
      "step: 119000, training_loss: 2.28871e+01\n",
      "step: 120000, training_loss: 2.08035e+01\n",
      "Average mmd : 7.08472549915129e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 121000, training_loss: 1.92482e+01\n",
      "step: 122000, training_loss: 4.01608e+01\n",
      "step: 123000, training_loss: 2.23597e+01\n",
      "step: 124000, training_loss: 1.68935e+01\n",
      "step: 125000, training_loss: 2.63937e+01\n",
      "step: 126000, training_loss: 1.91021e+01\n",
      "step: 127000, training_loss: 1.86300e+01\n",
      "step: 128000, training_loss: 2.03073e+01\n",
      "step: 129000, training_loss: 1.79578e+01\n",
      "step: 130000, training_loss: 2.17059e+01\n",
      "Average mmd : 3.0190679296770995e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 131000, training_loss: 1.83912e+01\n",
      "step: 132000, training_loss: 1.90754e+01\n",
      "step: 133000, training_loss: 2.00229e+01\n",
      "step: 134000, training_loss: 2.07455e+01\n",
      "step: 135000, training_loss: 2.31827e+01\n",
      "step: 136000, training_loss: 1.99798e+01\n",
      "step: 137000, training_loss: 2.67335e+01\n",
      "step: 138000, training_loss: 2.16678e+01\n",
      "step: 139000, training_loss: 2.60009e+01\n",
      "step: 140000, training_loss: 1.56220e+01\n",
      "Average mmd : 8.318851971389485e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 141000, training_loss: 1.98153e+01\n",
      "step: 142000, training_loss: 1.43805e+01\n",
      "step: 143000, training_loss: 1.83044e+01\n",
      "step: 144000, training_loss: 1.99401e+01\n",
      "step: 145000, training_loss: 1.77299e+01\n",
      "step: 146000, training_loss: 1.94549e+01\n",
      "step: 147000, training_loss: 2.24804e+01\n",
      "step: 148000, training_loss: 2.44748e+01\n",
      "step: 149000, training_loss: 1.97695e+01\n",
      "step: 150000, training_loss: 2.05691e+01\n",
      "Average mmd : 7.205973194390758e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 151000, training_loss: 1.87167e+01\n",
      "step: 152000, training_loss: 2.35711e+01\n",
      "step: 153000, training_loss: 1.55677e+01\n",
      "step: 154000, training_loss: 1.63002e+01\n",
      "step: 155000, training_loss: 2.25106e+01\n",
      "step: 156000, training_loss: 1.79228e+01\n",
      "step: 157000, training_loss: 1.77518e+01\n",
      "step: 158000, training_loss: 1.71027e+01\n",
      "step: 159000, training_loss: 1.69288e+01\n",
      "step: 160000, training_loss: 1.65109e+01\n",
      "Average mmd : -1.2365235741662595e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 161000, training_loss: 2.17977e+01\n",
      "step: 162000, training_loss: 2.14804e+01\n",
      "step: 163000, training_loss: 1.93641e+01\n",
      "step: 164000, training_loss: 2.27103e+01\n",
      "step: 165000, training_loss: 2.17447e+01\n",
      "step: 166000, training_loss: 2.04266e+01\n",
      "step: 167000, training_loss: 1.77442e+01\n",
      "step: 168000, training_loss: 1.84252e+01\n",
      "step: 169000, training_loss: 1.83157e+01\n",
      "step: 170000, training_loss: 1.93273e+01\n",
      "Average mmd : 5.682422043062907e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 171000, training_loss: 1.80365e+01\n",
      "step: 172000, training_loss: 1.92828e+01\n",
      "step: 173000, training_loss: 2.14613e+01\n",
      "step: 174000, training_loss: 1.93449e+01\n",
      "step: 175000, training_loss: 2.50170e+01\n",
      "step: 176000, training_loss: 2.16177e+01\n",
      "step: 177000, training_loss: 1.97483e+01\n",
      "step: 178000, training_loss: 1.93271e+01\n",
      "step: 179000, training_loss: 1.93184e+01\n",
      "step: 180000, training_loss: 1.86955e+01\n",
      "Average mmd : -1.8148298809417263e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 181000, training_loss: 2.59543e+01\n",
      "step: 182000, training_loss: 1.90063e+01\n",
      "step: 183000, training_loss: 1.89390e+01\n",
      "step: 184000, training_loss: 2.50664e+01\n",
      "step: 185000, training_loss: 2.20781e+01\n",
      "step: 186000, training_loss: 1.65911e+01\n",
      "step: 187000, training_loss: 1.85015e+01\n",
      "step: 188000, training_loss: 1.77582e+01\n",
      "step: 189000, training_loss: 2.84059e+01\n",
      "step: 190000, training_loss: 1.90760e+01\n",
      "Average mmd : 2.0296853412316018e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 191000, training_loss: 2.23102e+01\n",
      "step: 192000, training_loss: 1.73466e+01\n",
      "step: 193000, training_loss: 2.10173e+01\n",
      "step: 194000, training_loss: 1.91711e+01\n",
      "step: 195000, training_loss: 3.04460e+01\n",
      "step: 196000, training_loss: 1.98811e+01\n",
      "step: 197000, training_loss: 2.02583e+01\n",
      "step: 198000, training_loss: 2.65102e+01\n",
      "step: 199000, training_loss: 1.82702e+01\n",
      "step: 200000, training_loss: 1.71762e+01\n",
      "Average mmd : -3.322864373966894e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 201000, training_loss: 3.74201e+01\n",
      "step: 202000, training_loss: 1.68194e+01\n",
      "step: 203000, training_loss: 1.77708e+01\n",
      "step: 204000, training_loss: 1.85826e+01\n",
      "step: 205000, training_loss: 2.37129e+01\n",
      "step: 206000, training_loss: 1.87821e+01\n",
      "step: 207000, training_loss: 1.88308e+01\n",
      "step: 208000, training_loss: 2.03442e+01\n",
      "step: 209000, training_loss: 1.64164e+01\n",
      "step: 210000, training_loss: 1.80733e+01\n",
      "Average mmd : -7.027717269425526e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 211000, training_loss: 1.95369e+01\n",
      "step: 212000, training_loss: 1.88554e+01\n",
      "step: 213000, training_loss: 1.82109e+01\n",
      "step: 214000, training_loss: 2.13106e+01\n",
      "step: 215000, training_loss: 2.62926e+01\n",
      "step: 216000, training_loss: 2.09522e+01\n",
      "step: 217000, training_loss: 3.63382e+01\n",
      "step: 218000, training_loss: 1.92660e+01\n",
      "step: 219000, training_loss: 2.31504e+01\n",
      "step: 220000, training_loss: 1.79929e+01\n",
      "Average mmd : 9.376785618409045e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 221000, training_loss: 2.15940e+01\n",
      "step: 222000, training_loss: 1.87405e+01\n",
      "step: 223000, training_loss: 1.81374e+01\n",
      "step: 224000, training_loss: 1.92206e+01\n",
      "step: 225000, training_loss: 2.71982e+01\n",
      "step: 226000, training_loss: 1.71303e+01\n",
      "step: 227000, training_loss: 2.17340e+01\n",
      "step: 228000, training_loss: 1.94151e+01\n",
      "step: 229000, training_loss: 2.32967e+01\n",
      "step: 230000, training_loss: 1.67300e+01\n",
      "Average mmd : 0.00011119047215912836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 231000, training_loss: 2.55445e+01\n",
      "step: 232000, training_loss: 2.82795e+01\n",
      "step: 233000, training_loss: 1.97671e+01\n",
      "step: 234000, training_loss: 1.91936e+01\n",
      "step: 235000, training_loss: 2.13338e+01\n",
      "step: 236000, training_loss: 2.38660e+01\n",
      "step: 237000, training_loss: 1.84668e+01\n",
      "step: 238000, training_loss: 1.88737e+01\n",
      "step: 239000, training_loss: 2.08938e+01\n",
      "step: 240000, training_loss: 1.74645e+01\n",
      "Average mmd : -3.587005642780028e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 241000, training_loss: 2.61986e+01\n",
      "step: 242000, training_loss: 2.12288e+01\n",
      "step: 243000, training_loss: 2.25842e+01\n",
      "step: 244000, training_loss: 2.32395e+01\n",
      "step: 245000, training_loss: 1.94309e+01\n",
      "step: 246000, training_loss: 2.20262e+01\n",
      "step: 247000, training_loss: 2.16806e+01\n",
      "step: 248000, training_loss: 1.74307e+01\n",
      "step: 249000, training_loss: 2.14717e+01\n",
      "step: 250000, training_loss: 2.10141e+01\n",
      "Average mmd : -4.735319371451663e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 251000, training_loss: 1.89682e+01\n",
      "step: 252000, training_loss: 2.23543e+01\n",
      "step: 253000, training_loss: 2.50968e+01\n",
      "step: 254000, training_loss: 1.69513e+01\n",
      "step: 255000, training_loss: 1.82974e+01\n",
      "step: 256000, training_loss: 2.99302e+01\n",
      "step: 257000, training_loss: 1.76267e+01\n",
      "step: 258000, training_loss: 1.80767e+01\n",
      "step: 259000, training_loss: 1.82399e+01\n",
      "step: 260000, training_loss: 1.83938e+01\n",
      "Average mmd : -6.816454444824593e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 261000, training_loss: 2.09887e+01\n",
      "step: 262000, training_loss: 2.07003e+01\n",
      "step: 263000, training_loss: 1.87934e+01\n",
      "step: 264000, training_loss: 2.09138e+01\n",
      "step: 265000, training_loss: 1.83559e+01\n",
      "step: 266000, training_loss: 2.71851e+01\n",
      "step: 267000, training_loss: 2.13770e+01\n",
      "step: 268000, training_loss: 2.40124e+01\n",
      "step: 269000, training_loss: 1.86528e+01\n",
      "step: 270000, training_loss: 2.41552e+01\n",
      "Average mmd : 8.641048432711518e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 271000, training_loss: 1.88365e+01\n",
      "step: 272000, training_loss: 1.77788e+01\n",
      "step: 273000, training_loss: 2.04835e+01\n",
      "step: 274000, training_loss: 1.93604e+01\n",
      "step: 275000, training_loss: 2.51989e+01\n",
      "step: 276000, training_loss: 1.85279e+01\n",
      "step: 277000, training_loss: 1.86710e+01\n",
      "step: 278000, training_loss: 2.26047e+01\n",
      "step: 279000, training_loss: 2.19829e+01\n",
      "step: 280000, training_loss: 1.91292e+01\n",
      "Average mmd : 2.6044790146673158e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 281000, training_loss: 1.98236e+01\n",
      "step: 282000, training_loss: 1.76718e+01\n",
      "step: 283000, training_loss: 1.82537e+01\n",
      "step: 284000, training_loss: 1.95154e+01\n",
      "step: 285000, training_loss: 2.26115e+01\n",
      "step: 286000, training_loss: 1.74093e+01\n",
      "step: 287000, training_loss: 2.14619e+01\n",
      "step: 288000, training_loss: 2.59510e+01\n",
      "step: 289000, training_loss: 2.02672e+01\n",
      "step: 290000, training_loss: 2.32667e+01\n",
      "Average mmd : 2.009589560203473e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 291000, training_loss: 1.87541e+01\n",
      "step: 292000, training_loss: 2.72955e+01\n",
      "step: 293000, training_loss: 3.57964e+01\n",
      "step: 294000, training_loss: 2.20398e+01\n",
      "step: 295000, training_loss: 1.85206e+01\n",
      "step: 296000, training_loss: 1.66638e+01\n",
      "step: 297000, training_loss: 1.92094e+01\n",
      "step: 298000, training_loss: 1.80110e+01\n",
      "step: 299000, training_loss: 2.49247e+01\n",
      "step: 300000, training_loss: 1.86149e+01\n",
      "Average mmd : -2.437535099542032e-06\n"
     ]
    }
   ],
   "source": [
    "for step in range(initial_step,num_train_steps+1):\n",
    "#while state['step'] < num_train_steps + 1:\n",
    "    #step = state['step']\n",
    "    batch=next(train_iter).to(device)\n",
    "    # Compute loss using the simplified loss function\n",
    "    loss=train_step_fn(state, batch)\n",
    "    \n",
    "        \n",
    "    if step % log_freq == 0:\n",
    "        print(\"step: %d, training_loss: %.5e\" % (step, loss.mean().item()))\n",
    "            \n",
    "    if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "        # Save the checkpoint.\n",
    "        save_step = step // cfg.training.snapshot_freq\n",
    "        utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "        \n",
    "        #want to use the ema weights for sampling\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "\n",
    "        #print the metric used in Table1 of Lou2023. Should get at least as small as 1.62e-5 to be considered done training. \n",
    "        avg_mmd = 0.0\n",
    "        for i in range(eval_rounds):\n",
    "            gt_data = []\n",
    "            for _ in range(plot_samples // batch_size):\n",
    "                gt_data.append(next(train_ds).cpu().numpy())\n",
    "            gt_data = np.concatenate(gt_data, axis=0)\n",
    "            gt_data = np.reshape(gt_data, (-1, seq_len))\n",
    "            sample_data=[]\n",
    "            for _ in range(plot_samples // batch_size):\n",
    "                sample_data.append(sampling_fn(score_model).cpu().numpy())\n",
    "            sample_data=np.concatenate(sample_data,axis=0)\n",
    "            x0 = np.reshape(sample_data, gt_data.shape)\n",
    "            mmd = binary_exp_hamming_mmd(x0, gt_data)\n",
    "            avg_mmd += mmd\n",
    "            model_helper.plot(x0,os.path.join(sample_dir,f'step_{save_step}_eval_round_{i}.pdf')) #plot a sample from the model\n",
    "\n",
    "        avg_mmd = avg_mmd / eval_rounds\n",
    "        print(f'Average mmd : {avg_mmd}')\n",
    "        with open('output.txt', 'a') as file:\n",
    "            file.write(f'time: {datetime.now()},step: {save_step}, loss: {loss.mean().item()}, MMD: {avg_mmd}\\n')\n",
    "            \n",
    "        ema.restore(score_model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
