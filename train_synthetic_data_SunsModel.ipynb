{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data.synthetic import utils as data_utils\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import functools\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def transformer_timestep_embedding(timesteps, dim, device,max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings (like in transformer position encodings).\n",
    "    timesteps: (batch,) or (N,)\n",
    "    Returns: (batch, dim)\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(0, half, dtype=torch.float32,device=device) / half)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:  # zero pad if needed\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class CatMLPScoreFunc(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len,cat_embed_size, num_layers, hidden_size, time_scale_factor=1000.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cat_embed_size = cat_embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_scale_factor = time_scale_factor\n",
    "        input_dim=cat_embed_size*seq_len\n",
    "        self.input_dim=input_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, cat_embed_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim,hidden_size))  # will init in forward\n",
    "        for _ in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(hidden_size,hidden_size))  # will init in forward\n",
    "        self.final = nn.Linear(hidden_size, seq_len*vocab_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) – categorical token ids\n",
    "        t: (batch,) – timesteps\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        V=self.vocab_size\n",
    "        x = self.embed(x)  # (B, L, cat_embed_size)\n",
    "        x = x.view(B, -1)  # (B, L * cat_embed_size)\n",
    "\n",
    "        temb = transformer_timestep_embedding(t * self.time_scale_factor, self.hidden_size,x.device).to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + temb\n",
    "            x = F.silu(x)\n",
    "\n",
    "        x = self.final(x)  # (B, L*vocab_size)\n",
    "        x=x.view(B, L, V)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_directory='data//synthetic//checkerboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices.\n",
      "NVIDIA GeForce GTX 1080 \t Memory: 8.00GB\n",
      "Number of parameters in the model: 2254432\n",
      "CatMLPScoreFunc(\n",
      "  (embed): Embedding(3, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (final): Linear(in_features=256, out_features=96, bias=True)\n",
      ")\n",
      "EMA: <model.ema.ExponentialMovingAverage object at 0x00000140A46BBDC0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_21496\\1578425410.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "2025-04-21 18:51:50,082 - No checkpoint found at for_synthetic_data_mixed\\checkpoints-meta\\checkpoint.pth. Returned the same state as input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-06\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "Scaler: <torch.cuda.amp.grad_scaler.GradScaler object at 0x00000140A46D4190>\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import utils\n",
    "from model.ema import ExponentialMovingAverage\n",
    "import losses\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "graph_type='mixed' #uniform, masked, or mixed\n",
    "\n",
    "\n",
    "cfg_path=f'configs//synthetic_config_{graph_type}.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//tiny.yaml')\n",
    "work_dir = f'for_synthetic_data_{graph_type}'\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "# Create directories for experimental logs\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "utils.makedirs(sample_dir)\n",
    "utils.makedirs(checkpoint_dir)\n",
    "utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"WARNING: Using device {}\".format(device))\n",
    "    print(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "# build token graph\n",
    "graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "# build score model\n",
    "batch_size = cfg.training.batch_size\n",
    "seq_len=32\n",
    "vocab_size = 2\n",
    "mask_id = vocab_size\n",
    "if graph_type=='uniform':\n",
    "    expand_vocab_size = vocab_size \n",
    "else:\n",
    "    expand_vocab_size = vocab_size + 1\n",
    "time_scale_factor=1000.0\n",
    "embed_dim=256\n",
    "num_layers=3\n",
    "\n",
    "score_model = CatMLPScoreFunc(vocab_size=expand_vocab_size,cat_embed_size=embed_dim,num_layers= num_layers,\n",
    "    hidden_size=embed_dim,seq_len=seq_len,time_scale_factor= 1000.0,\n",
    "    ).to(device)\n",
    "\n",
    "num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")\n",
    "ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "print(score_model)\n",
    "print(f\"EMA: {ema}\")\n",
    "\n",
    "# build noise\n",
    "noise = noise_lib.get_noise(cfg).to(device)\n",
    "#noise = DDP(noise, device_ids=[rank], static_graph=True) Z:Commented this out\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "# build optimization state\n",
    "optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f\"Scaler: {scaler}\")\n",
    "state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (10000000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from file - assumes we have already generated samples using generate_data.ipynb\n",
    "data_file = os.path.join(data_directory, 'data.npy')\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = np.load(f).astype(np.int64)\n",
    "    print('data shape: %s' % str(data.shape))\n",
    "\n",
    "# Define a custom Dataset to wrap the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create a Dataset instance\n",
    "train_set = CustomDataset(data)\n",
    "\n",
    "# Function to cycle through DataLoader\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "# Initialize DataLoader without a sampler\n",
    "train_ds = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,  # Shuffle the data as needed\n",
    "    persistent_workers=False,\n",
    "))\n",
    "\n",
    "# Create an iterator for the data\n",
    "train_iter = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n"
     ]
    }
   ],
   "source": [
    "#plotting code borrowed from Sun2023\n",
    "\n",
    "def plot(xbin, fn_xbin2float, output_file=None):\n",
    "  \"\"\"Visualize binary data.\"\"\"\n",
    "  float_data = fn_xbin2float(xbin)\n",
    "  if output_file is None:  # in-memory plot\n",
    "    buf = io.BytesIO()\n",
    "    data_utils.plot_samples(float_data, buf, im_size=4.1, im_fmt='png')\n",
    "    buf.seek(0)\n",
    "    image = np.asarray(PIL.Image.open(buf))[None, ...]\n",
    "    return image\n",
    "  else:\n",
    "    with open(output_file, 'wb') as f:\n",
    "      im_fmt = 'png' if output_file.endswith('.png') else 'pdf'\n",
    "      data_utils.plot_samples(float_data, f, im_size=4.1, im_fmt=im_fmt,axis=True)\n",
    "\n",
    "\n",
    "class BinarySyntheticHelper(object):\n",
    "  \"\"\"Binary synthetic model helper.\"\"\"\n",
    "\n",
    "  def __init__(self, seq_len,int_scale):\n",
    "    self.seq_len = seq_len\n",
    "    self.int_scale=int_scale\n",
    "    self.bm, self.inv_bm = data_utils.get_binmap(seq_len,\n",
    "                                                 'gray')\n",
    "\n",
    "  def plot(self, xbin, output_file=None):\n",
    "    fn_xbin2float = functools.partial(\n",
    "        data_utils.bin2float, inv_bm=self.inv_bm,\n",
    "        discrete_dim=self.seq_len, int_scale=self.int_scale)\n",
    "    return plot(xbin, fn_xbin2float, output_file)\n",
    "\n",
    "int_scale=5461.760975376213\n",
    "model_helper = BinarySyntheticHelper(seq_len,int_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metric used in Table 1 of Lou2023\n",
    "def binary_mmd(x, y, sim_fn):\n",
    "  \"\"\"MMD for binary data.\"\"\"\n",
    "  x = x.astype(np.float32)\n",
    "  y = y.astype(np.float32)\n",
    "  kxx = sim_fn(x, x)\n",
    "  kxx = kxx * (1 - np.eye(x.shape[0]))\n",
    "  kxx = np.sum(kxx) / x.shape[0] / (x.shape[0] - 1)\n",
    "\n",
    "  kyy = sim_fn(y, y)\n",
    "  kyy = kyy * (1 - np.eye(y.shape[0]))\n",
    "  kyy = np.sum(kyy) / y.shape[0] / (y.shape[0] - 1)\n",
    "  kxy = np.sum(sim_fn(x, y))\n",
    "  kxy = kxy / x.shape[0] / y.shape[0]\n",
    "  mmd = kxx + kyy - 2 * kxy\n",
    "  return mmd\n",
    "\n",
    "def binary_exp_hamming_sim(x, y, bd):\n",
    "  x = np.expand_dims(x, axis=1)\n",
    "  y = np.expand_dims(y, axis=0)\n",
    "  d = np.sum(np.abs(x - y), axis=-1)\n",
    "  return np.exp(-bd * d)\n",
    "\n",
    "def binary_exp_hamming_mmd(x, y, bandwidth=0.1):\n",
    "  sim_fn = functools.partial(binary_exp_hamming_sim, bd=bandwidth)\n",
    "  return binary_mmd(x, y, sim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at step 0.\n"
     ]
    }
   ],
   "source": [
    "num_train_steps=cfg.training.n_iters\n",
    "log_freq=cfg.training.log_freq\n",
    "snapshot_freq=cfg.training.snapshot_freq\n",
    "eval_rounds=1\n",
    "plot_samples=cfg.plot_samples\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(cfg)\n",
    "train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "num_train_steps = cfg.training.n_iters\n",
    "initial_step = int(state['step'])\n",
    "print(f\"Starting training loop at step {initial_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training_loss: 1.71482e+01\n",
      "step: 1000, training_loss: 7.56344e+00\n",
      "step: 2000, training_loss: 7.38736e+00\n",
      "step: 3000, training_loss: 6.21957e+00\n",
      "step: 4000, training_loss: 7.24757e+00\n",
      "step: 5000, training_loss: 7.60618e+00\n",
      "step: 6000, training_loss: 7.50251e+00\n",
      "step: 7000, training_loss: 9.82674e+00\n",
      "step: 8000, training_loss: 6.46239e+00\n",
      "step: 9000, training_loss: 8.39887e+00\n",
      "step: 10000, training_loss: 7.53453e+00\n",
      "Average mmd : 0.008014215259827373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11000, training_loss: 6.20032e+00\n",
      "step: 12000, training_loss: 5.55305e+00\n",
      "step: 13000, training_loss: 8.70048e+00\n",
      "step: 14000, training_loss: 6.91900e+00\n",
      "step: 15000, training_loss: 7.50054e+00\n",
      "step: 16000, training_loss: 7.99133e+00\n",
      "step: 17000, training_loss: 8.19992e+00\n",
      "step: 18000, training_loss: 6.02326e+00\n",
      "step: 19000, training_loss: 6.65472e+00\n",
      "step: 20000, training_loss: 5.56809e+00\n",
      "Average mmd : 0.006183738895954027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21000, training_loss: 5.24581e+00\n",
      "step: 22000, training_loss: 6.28522e+00\n",
      "step: 23000, training_loss: 5.63971e+00\n",
      "step: 24000, training_loss: 5.14239e+00\n",
      "step: 25000, training_loss: 5.73482e+00\n",
      "step: 26000, training_loss: 6.30408e+00\n",
      "step: 27000, training_loss: 7.19137e+00\n",
      "step: 28000, training_loss: 5.68646e+00\n",
      "step: 29000, training_loss: 1.13469e+01\n",
      "step: 30000, training_loss: 5.94672e+00\n",
      "Average mmd : 0.005127112353450314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31000, training_loss: 8.28453e+00\n",
      "step: 32000, training_loss: 5.42605e+00\n",
      "step: 33000, training_loss: 7.10979e+00\n",
      "step: 34000, training_loss: 4.94279e+00\n",
      "step: 35000, training_loss: 5.16012e+00\n",
      "step: 36000, training_loss: 7.05031e+00\n",
      "step: 37000, training_loss: 4.96110e+00\n",
      "step: 38000, training_loss: 9.43828e+00\n",
      "step: 39000, training_loss: 5.00266e+00\n",
      "step: 40000, training_loss: 6.73908e+00\n",
      "Average mmd : 0.004143952856917599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41000, training_loss: 5.83709e+00\n",
      "step: 42000, training_loss: 6.91426e+00\n",
      "step: 43000, training_loss: 4.69429e+00\n",
      "step: 44000, training_loss: 6.28773e+00\n",
      "step: 45000, training_loss: 6.68255e+00\n",
      "step: 46000, training_loss: 6.14435e+00\n",
      "step: 47000, training_loss: 5.97496e+00\n",
      "step: 48000, training_loss: 8.47275e+00\n",
      "step: 49000, training_loss: 6.89346e+00\n",
      "step: 50000, training_loss: 6.67046e+00\n",
      "Average mmd : 0.004575772925793142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51000, training_loss: 6.07232e+00\n",
      "step: 52000, training_loss: 7.39342e+00\n",
      "step: 53000, training_loss: 5.48835e+00\n",
      "step: 54000, training_loss: 9.65886e+00\n",
      "step: 55000, training_loss: 5.84599e+00\n",
      "step: 56000, training_loss: 5.10499e+00\n",
      "step: 57000, training_loss: 5.68367e+00\n",
      "step: 58000, training_loss: 5.74119e+00\n",
      "step: 59000, training_loss: 6.80108e+00\n",
      "step: 60000, training_loss: 7.66481e+00\n",
      "Average mmd : 0.004514003534140776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61000, training_loss: 5.72118e+00\n",
      "step: 62000, training_loss: 6.78144e+00\n",
      "step: 63000, training_loss: 5.53180e+00\n",
      "step: 64000, training_loss: 4.87567e+00\n",
      "step: 65000, training_loss: 8.20247e+00\n",
      "step: 66000, training_loss: 6.39982e+00\n",
      "step: 67000, training_loss: 1.13396e+01\n",
      "step: 68000, training_loss: 6.64314e+00\n",
      "step: 69000, training_loss: 6.53169e+00\n",
      "step: 70000, training_loss: 7.70550e+00\n",
      "Average mmd : 0.005099337407798943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 71000, training_loss: 7.86062e+00\n",
      "step: 72000, training_loss: 5.79530e+00\n",
      "step: 73000, training_loss: 7.30661e+00\n",
      "step: 74000, training_loss: 7.95391e+00\n",
      "step: 75000, training_loss: 8.81277e+00\n",
      "step: 76000, training_loss: 7.53140e+00\n",
      "step: 77000, training_loss: 7.14815e+00\n",
      "step: 78000, training_loss: 5.99551e+00\n",
      "step: 79000, training_loss: 6.14385e+00\n",
      "step: 80000, training_loss: 6.36565e+00\n",
      "Average mmd : 0.0042862445600751475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 81000, training_loss: 6.86002e+00\n",
      "step: 82000, training_loss: 7.01986e+00\n",
      "step: 83000, training_loss: 6.48730e+00\n",
      "step: 84000, training_loss: 6.09210e+00\n",
      "step: 85000, training_loss: 5.76579e+00\n",
      "step: 86000, training_loss: 5.80154e+00\n",
      "step: 87000, training_loss: 7.96791e+00\n",
      "step: 88000, training_loss: 5.62312e+00\n",
      "step: 89000, training_loss: 7.18913e+00\n",
      "step: 90000, training_loss: 7.71258e+00\n",
      "Average mmd : 0.004098657170211739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 91000, training_loss: 5.51851e+00\n",
      "step: 92000, training_loss: 6.09230e+00\n",
      "step: 93000, training_loss: 7.66061e+00\n",
      "step: 94000, training_loss: 7.52122e+00\n",
      "step: 95000, training_loss: 8.76316e+00\n",
      "step: 96000, training_loss: 5.80143e+00\n",
      "step: 97000, training_loss: 1.18751e+01\n",
      "step: 98000, training_loss: 6.87599e+00\n",
      "step: 99000, training_loss: 6.90630e+00\n",
      "step: 100000, training_loss: 5.99662e+00\n",
      "Average mmd : 0.004875880921448583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 101000, training_loss: 6.19692e+00\n",
      "step: 102000, training_loss: 6.72991e+00\n",
      "step: 103000, training_loss: 6.48671e+00\n",
      "step: 104000, training_loss: 6.53297e+00\n",
      "step: 105000, training_loss: 6.48122e+00\n",
      "step: 106000, training_loss: 8.79349e+00\n",
      "step: 107000, training_loss: 5.41652e+00\n",
      "step: 108000, training_loss: 5.59052e+00\n",
      "step: 109000, training_loss: 7.61667e+00\n",
      "step: 110000, training_loss: 5.01813e+00\n",
      "Average mmd : 0.005549620991961768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 111000, training_loss: 6.71742e+00\n",
      "step: 112000, training_loss: 5.74333e+00\n",
      "step: 113000, training_loss: 5.14003e+00\n",
      "step: 114000, training_loss: 4.82829e+00\n",
      "step: 115000, training_loss: 5.64233e+00\n",
      "step: 116000, training_loss: 6.86800e+00\n",
      "step: 117000, training_loss: 7.07904e+00\n",
      "step: 118000, training_loss: 8.38668e+00\n",
      "step: 119000, training_loss: 6.26332e+00\n",
      "step: 120000, training_loss: 7.00603e+00\n",
      "Average mmd : 0.0055802792701933335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 121000, training_loss: 6.59375e+00\n",
      "step: 122000, training_loss: 7.51922e+00\n",
      "step: 123000, training_loss: 6.95140e+00\n",
      "step: 124000, training_loss: 9.80435e+00\n",
      "step: 125000, training_loss: 4.68892e+00\n",
      "step: 126000, training_loss: 5.65260e+00\n",
      "step: 127000, training_loss: 5.41659e+00\n",
      "step: 128000, training_loss: 7.78098e+00\n",
      "step: 129000, training_loss: 6.44449e+00\n",
      "step: 130000, training_loss: 5.68276e+00\n",
      "Average mmd : 0.005255278781293005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 131000, training_loss: 4.20282e+00\n",
      "step: 132000, training_loss: 8.50259e+00\n",
      "step: 133000, training_loss: 8.23747e+00\n",
      "step: 134000, training_loss: 5.64922e+00\n",
      "step: 135000, training_loss: 1.01628e+01\n",
      "step: 136000, training_loss: 7.24032e+00\n",
      "step: 137000, training_loss: 5.50666e+00\n",
      "step: 138000, training_loss: 6.98588e+00\n",
      "step: 139000, training_loss: 8.30356e+00\n",
      "step: 140000, training_loss: 8.10367e+00\n",
      "Average mmd : 0.005060532787630023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 141000, training_loss: 5.91157e+00\n",
      "step: 142000, training_loss: 4.80830e+00\n",
      "step: 143000, training_loss: 7.09075e+00\n",
      "step: 144000, training_loss: 7.48178e+00\n",
      "step: 145000, training_loss: 6.33236e+00\n",
      "step: 146000, training_loss: 8.68794e+00\n",
      "step: 147000, training_loss: 8.68512e+00\n",
      "step: 148000, training_loss: 7.30269e+00\n",
      "step: 149000, training_loss: 5.57020e+00\n",
      "step: 150000, training_loss: 5.81595e+00\n",
      "Average mmd : 0.0056356134093047405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 151000, training_loss: 6.71158e+00\n",
      "step: 152000, training_loss: 6.25601e+00\n",
      "step: 153000, training_loss: 5.92863e+00\n",
      "step: 154000, training_loss: 8.13591e+00\n",
      "step: 155000, training_loss: 6.51802e+00\n",
      "step: 156000, training_loss: 6.96736e+00\n",
      "step: 157000, training_loss: 7.22419e+00\n",
      "step: 158000, training_loss: 7.76303e+00\n",
      "step: 159000, training_loss: 6.59953e+00\n",
      "step: 160000, training_loss: 7.04725e+00\n",
      "Average mmd : 0.0055915635959540855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 161000, training_loss: 5.25667e+00\n",
      "step: 162000, training_loss: 9.68425e+00\n",
      "step: 163000, training_loss: 7.47592e+00\n",
      "step: 164000, training_loss: 6.67079e+00\n",
      "step: 165000, training_loss: 5.43872e+00\n",
      "step: 166000, training_loss: 5.41939e+00\n",
      "step: 167000, training_loss: 6.28497e+00\n",
      "step: 168000, training_loss: 6.48529e+00\n",
      "step: 169000, training_loss: 5.32108e+00\n",
      "step: 170000, training_loss: 5.52277e+00\n",
      "Average mmd : 0.006086738169605865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 171000, training_loss: 8.03352e+00\n",
      "step: 172000, training_loss: 7.21258e+00\n",
      "step: 173000, training_loss: 5.76316e+00\n",
      "step: 174000, training_loss: 6.49017e+00\n",
      "step: 175000, training_loss: 9.95176e+00\n",
      "step: 176000, training_loss: 7.28934e+00\n",
      "step: 177000, training_loss: 8.18254e+00\n",
      "step: 178000, training_loss: 4.46652e+00\n",
      "step: 179000, training_loss: 5.37013e+00\n",
      "step: 180000, training_loss: 8.12510e+00\n",
      "Average mmd : 0.006452796496756896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 181000, training_loss: 5.63393e+00\n",
      "step: 182000, training_loss: 5.62833e+00\n",
      "step: 183000, training_loss: 1.13450e+01\n",
      "step: 184000, training_loss: 6.22040e+00\n",
      "step: 185000, training_loss: 5.29311e+00\n",
      "step: 186000, training_loss: 6.74946e+00\n",
      "step: 187000, training_loss: 8.16230e+00\n",
      "step: 188000, training_loss: 6.11681e+00\n",
      "step: 189000, training_loss: 1.12089e+01\n",
      "step: 190000, training_loss: 7.59340e+00\n",
      "Average mmd : 0.0063040166436793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 191000, training_loss: 6.00727e+00\n",
      "step: 192000, training_loss: 4.65562e+00\n",
      "step: 193000, training_loss: 5.71405e+00\n",
      "step: 194000, training_loss: 6.38425e+00\n",
      "step: 195000, training_loss: 6.78953e+00\n",
      "step: 196000, training_loss: 6.33279e+00\n",
      "step: 197000, training_loss: 5.08725e+00\n",
      "step: 198000, training_loss: 9.66574e+00\n",
      "step: 199000, training_loss: 6.70251e+00\n",
      "step: 200000, training_loss: 5.92803e+00\n",
      "Average mmd : 0.006165108258018359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 201000, training_loss: 1.15165e+01\n",
      "step: 202000, training_loss: 6.07237e+00\n",
      "step: 203000, training_loss: 7.09740e+00\n",
      "step: 204000, training_loss: 6.69464e+00\n",
      "step: 205000, training_loss: 6.28498e+00\n",
      "step: 206000, training_loss: 6.29545e+00\n",
      "step: 207000, training_loss: 6.52742e+00\n",
      "step: 208000, training_loss: 7.01406e+00\n",
      "step: 209000, training_loss: 7.05415e+00\n",
      "step: 210000, training_loss: 6.86799e+00\n",
      "Average mmd : 0.006842140646523498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 211000, training_loss: 6.57255e+00\n",
      "step: 212000, training_loss: 6.04637e+00\n",
      "step: 213000, training_loss: 1.42476e+01\n",
      "step: 214000, training_loss: 5.09158e+00\n",
      "step: 215000, training_loss: 6.04804e+00\n",
      "step: 216000, training_loss: 5.60534e+00\n",
      "step: 217000, training_loss: 4.80957e+00\n",
      "step: 218000, training_loss: 6.15242e+00\n",
      "step: 219000, training_loss: 5.80425e+00\n",
      "step: 220000, training_loss: 5.36893e+00\n",
      "Average mmd : 0.006704628274250335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 221000, training_loss: 6.52713e+00\n",
      "step: 222000, training_loss: 6.13855e+00\n",
      "step: 223000, training_loss: 6.13467e+00\n",
      "step: 224000, training_loss: 7.67262e+00\n",
      "step: 225000, training_loss: 5.71007e+00\n",
      "step: 226000, training_loss: 7.31298e+00\n",
      "step: 227000, training_loss: 7.78920e+00\n",
      "step: 228000, training_loss: 8.97919e+00\n",
      "step: 229000, training_loss: 6.33180e+00\n",
      "step: 230000, training_loss: 6.66581e+00\n",
      "Average mmd : 0.007195369014367681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 231000, training_loss: 9.54192e+00\n",
      "step: 232000, training_loss: 7.09519e+00\n",
      "step: 233000, training_loss: 7.10993e+00\n",
      "step: 234000, training_loss: 4.93627e+00\n",
      "step: 235000, training_loss: 5.87007e+00\n",
      "step: 236000, training_loss: 7.17744e+00\n",
      "step: 237000, training_loss: 6.73492e+00\n",
      "step: 238000, training_loss: 5.93736e+00\n",
      "step: 239000, training_loss: 6.10162e+00\n",
      "step: 240000, training_loss: 7.03871e+00\n",
      "Average mmd : 0.006454251440129122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 241000, training_loss: 4.65923e+00\n",
      "step: 242000, training_loss: 6.31370e+00\n",
      "step: 243000, training_loss: 9.97977e+00\n",
      "step: 244000, training_loss: 8.21424e+00\n",
      "step: 245000, training_loss: 5.05551e+00\n",
      "step: 246000, training_loss: 1.55409e+01\n",
      "step: 247000, training_loss: 5.14649e+00\n",
      "step: 248000, training_loss: 5.15760e+00\n",
      "step: 249000, training_loss: 1.04008e+01\n",
      "step: 250000, training_loss: 5.41006e+00\n",
      "Average mmd : 0.0067997103632913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 251000, training_loss: 4.25668e+00\n",
      "step: 252000, training_loss: 5.66461e+00\n",
      "step: 253000, training_loss: 7.52434e+00\n",
      "step: 254000, training_loss: 5.98570e+00\n",
      "step: 255000, training_loss: 6.20129e+00\n",
      "step: 256000, training_loss: 7.13139e+00\n",
      "step: 257000, training_loss: 6.76070e+00\n",
      "step: 258000, training_loss: 6.86370e+00\n",
      "step: 259000, training_loss: 7.74012e+00\n",
      "step: 260000, training_loss: 9.28472e+00\n",
      "Average mmd : 0.006375067448422089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 261000, training_loss: 6.94636e+00\n",
      "step: 262000, training_loss: 8.44631e+00\n",
      "step: 263000, training_loss: 6.16020e+00\n",
      "step: 264000, training_loss: 5.72176e+00\n",
      "step: 265000, training_loss: 5.40070e+00\n",
      "step: 266000, training_loss: 7.17984e+00\n",
      "step: 267000, training_loss: 6.37594e+00\n",
      "step: 268000, training_loss: 5.91050e+00\n",
      "step: 269000, training_loss: 5.71467e+00\n",
      "step: 270000, training_loss: 7.44411e+00\n",
      "Average mmd : 0.006250351471408178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 271000, training_loss: 5.95332e+00\n",
      "step: 272000, training_loss: 7.86274e+00\n",
      "step: 273000, training_loss: 6.24064e+00\n",
      "step: 274000, training_loss: 7.34550e+00\n",
      "step: 275000, training_loss: 5.67584e+00\n",
      "step: 276000, training_loss: 7.07193e+00\n",
      "step: 277000, training_loss: 6.87819e+00\n",
      "step: 278000, training_loss: 5.44106e+00\n",
      "step: 279000, training_loss: 6.16385e+00\n",
      "step: 280000, training_loss: 6.69821e+00\n",
      "Average mmd : 0.006746526415710552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 281000, training_loss: 8.13327e+00\n",
      "step: 282000, training_loss: 7.64615e+00\n",
      "step: 283000, training_loss: 2.52936e+01\n",
      "step: 284000, training_loss: 5.13257e+00\n",
      "step: 285000, training_loss: 6.95082e+00\n",
      "step: 286000, training_loss: 5.01486e+00\n",
      "step: 287000, training_loss: 7.13616e+00\n",
      "step: 288000, training_loss: 6.52189e+00\n",
      "step: 289000, training_loss: 7.85740e+00\n",
      "step: 290000, training_loss: 6.25495e+00\n",
      "Average mmd : 0.00759036481226838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 291000, training_loss: 4.73005e+00\n",
      "step: 292000, training_loss: 6.62744e+00\n",
      "step: 293000, training_loss: 8.96135e+00\n",
      "step: 294000, training_loss: 6.60660e+00\n",
      "step: 295000, training_loss: 7.99044e+00\n",
      "step: 296000, training_loss: 6.78157e+00\n",
      "step: 297000, training_loss: 5.61602e+00\n",
      "step: 298000, training_loss: 7.39843e+00\n",
      "step: 299000, training_loss: 5.26534e+00\n",
      "step: 300000, training_loss: 4.51206e+00\n",
      "Average mmd : 0.006221866321461489\n"
     ]
    }
   ],
   "source": [
    "for step in range(initial_step,num_train_steps+1):\n",
    "#while state['step'] < num_train_steps + 1:\n",
    "    #step = state['step']\n",
    "    batch=next(train_iter).to(device)\n",
    "    # Compute loss using the simplified loss function\n",
    "    loss=train_step_fn(state, batch)\n",
    "    \n",
    "        \n",
    "    if step % log_freq == 0:\n",
    "        print(\"step: %d, training_loss: %.5e\" % (step, loss.mean().item()))\n",
    "            \n",
    "    if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "        # Save the checkpoint.\n",
    "        save_step = step // cfg.training.snapshot_freq\n",
    "        utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "        \n",
    "        #want to use the ema weights for sampling\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "\n",
    "        #print the metric used in Table1 of Lou2023. Should get at least as small as 1.62e-5 to be considered done training. \n",
    "        avg_mmd = 0.0\n",
    "        for i in range(eval_rounds):\n",
    "            gt_data = []\n",
    "            for _ in range(plot_samples // batch_size):\n",
    "                gt_data.append(next(train_ds).cpu().numpy())\n",
    "            gt_data = np.concatenate(gt_data, axis=0)\n",
    "            gt_data = np.reshape(gt_data, (-1, seq_len))\n",
    "            sample_data=[]\n",
    "            for _ in range(plot_samples // batch_size):\n",
    "                sample_data.append(sampling_fn(score_model).cpu().numpy())\n",
    "            sample_data=np.concatenate(sample_data,axis=0)\n",
    "            x0 = np.reshape(sample_data, gt_data.shape)\n",
    "            mmd = binary_exp_hamming_mmd(x0, gt_data)\n",
    "            avg_mmd += mmd\n",
    "            model_helper.plot(x0,os.path.join(sample_dir,f'step_{save_step}_eval_round_{i}.pdf')) #plot a sample from the model\n",
    "\n",
    "        avg_mmd = avg_mmd / eval_rounds\n",
    "        print(f'Average mmd : {avg_mmd}')\n",
    "        with open('output.txt', 'a') as file:\n",
    "            file.write(f'time: {datetime.now()},step: {save_step}, loss: {loss.mean().item()}, MMD: {avg_mmd}\\n')\n",
    "            \n",
    "        ema.restore(score_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
