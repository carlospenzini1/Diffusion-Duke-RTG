{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\miniconda3\\envs\\sedd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data.synthetic import utils as data_utils\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import functools\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def transformer_timestep_embedding(timesteps, dim, device,max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings (like in transformer position encodings).\n",
    "    timesteps: (batch,) or (N,)\n",
    "    Returns: (batch, dim)\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(0, half, dtype=torch.float32,device=device) / half)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:  # zero pad if needed\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb\n",
    "\n",
    "\n",
    "class CatMLPScoreFunc(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len,cat_embed_size, num_layers, hidden_size, time_scale_factor=1000.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cat_embed_size = cat_embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_scale_factor = time_scale_factor\n",
    "        input_dim=cat_embed_size*seq_len\n",
    "        self.input_dim=input_dim\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, cat_embed_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim,hidden_size))  # will init in forward\n",
    "        for _ in range(num_layers-1):\n",
    "            self.layers.append(nn.Linear(hidden_size,hidden_size))  # will init in forward\n",
    "        self.final = nn.Linear(hidden_size, seq_len*vocab_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) – categorical token ids\n",
    "        t: (batch,) – timesteps\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        V=self.vocab_size\n",
    "        x = self.embed(x)  # (B, L, cat_embed_size)\n",
    "        x = x.view(B, -1)  # (B, L * cat_embed_size)\n",
    "\n",
    "        temb = transformer_timestep_embedding(t * self.time_scale_factor, self.hidden_size,x.device).to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + temb\n",
    "            x = F.silu(x)\n",
    "\n",
    "        x = self.final(x)  # (B, L*vocab_size)\n",
    "        x=x.view(B, L, V)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_directory='data//synthetic//checkerboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices.\n",
      "NVIDIA GeForce GTX 1080 \t Memory: 8.00GB\n",
      "Number of parameters in the model: 2254432\n",
      "CatMLPScoreFunc(\n",
      "  (embed): Embedding(3, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (final): Linear(in_features=256, out_features=96, bias=True)\n",
      ")\n",
      "EMA: <model.ema.ExponentialMovingAverage object at 0x00000225AAB2C220>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezem\\AppData\\Local\\Temp\\ipykernel_2976\\398646880.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "2025-04-23 12:58:07,035 - No checkpoint found at for_synthetic_data_mixed_nolog_loss\\checkpoints-meta\\checkpoint.pth. Returned the same state as input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-06\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 1e-06\n",
      ")\n",
      "Scaler: <torch.cuda.amp.grad_scaler.GradScaler object at 0x00000225AAB2C490>\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import utils\n",
    "from model.ema import ExponentialMovingAverage\n",
    "import losses\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "graph_type='mixed' #uniform, masked, or mixed\n",
    "\n",
    "\n",
    "cfg_path=f'configs//synthetic_config_{graph_type}.yaml'\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.model=OmegaConf.load('configs//model//tiny.yaml')\n",
    "work_dir = f'for_synthetic_data_{graph_type}_nolog_loss'\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "# Create directories for experimental logs\n",
    "sample_dir = os.path.join(work_dir, \"samples\")\n",
    "checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "utils.makedirs(sample_dir)\n",
    "utils.makedirs(checkpoint_dir)\n",
    "utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "device = torch.device(f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "else:\n",
    "    print(\"WARNING: Using device {}\".format(device))\n",
    "    print(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "# build token graph\n",
    "graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "# build score model\n",
    "batch_size = cfg.training.batch_size\n",
    "seq_len=32\n",
    "vocab_size = 2\n",
    "mask_id = vocab_size\n",
    "if graph_type=='uniform':\n",
    "    expand_vocab_size = vocab_size \n",
    "else:\n",
    "    expand_vocab_size = vocab_size + 1\n",
    "time_scale_factor=1000.0\n",
    "embed_dim=256\n",
    "num_layers=3\n",
    "\n",
    "score_model = CatMLPScoreFunc(vocab_size=expand_vocab_size,cat_embed_size=embed_dim,num_layers= num_layers,\n",
    "    hidden_size=embed_dim,seq_len=seq_len,time_scale_factor= 1000.0,\n",
    "    ).to(device)\n",
    "\n",
    "num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_parameters}\")\n",
    "ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "print(score_model)\n",
    "print(f\"EMA: {ema}\")\n",
    "\n",
    "# build noise\n",
    "noise = noise_lib.get_noise(cfg).to(device)\n",
    "#noise = DDP(noise, device_ids=[rank], static_graph=True) Z:Commented this out\n",
    "sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "# build optimization state\n",
    "optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f\"Scaler: {scaler}\")\n",
    "state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (10000000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from file - assumes we have already generated samples using generate_data.ipynb\n",
    "data_file = os.path.join(data_directory, 'data.npy')\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = np.load(f).astype(np.int64)\n",
    "    print('data shape: %s' % str(data.shape))\n",
    "\n",
    "# Define a custom Dataset to wrap the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create a Dataset instance\n",
    "train_set = CustomDataset(data)\n",
    "\n",
    "# Function to cycle through DataLoader\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "# Initialize DataLoader without a sampler\n",
    "train_ds = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,  # Shuffle the data as needed\n",
    "    persistent_workers=False,\n",
    "))\n",
    "\n",
    "# Create an iterator for the data\n",
    "train_iter = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remapping binary repr with gray code\n"
     ]
    }
   ],
   "source": [
    "#plotting code borrowed from Sun2023\n",
    "\n",
    "def plot(xbin, fn_xbin2float, output_file=None):\n",
    "  \"\"\"Visualize binary data.\"\"\"\n",
    "  float_data = fn_xbin2float(xbin)\n",
    "  if output_file is None:  # in-memory plot\n",
    "    buf = io.BytesIO()\n",
    "    data_utils.plot_samples(float_data, buf, im_size=4.1, im_fmt='png')\n",
    "    buf.seek(0)\n",
    "    image = np.asarray(PIL.Image.open(buf))[None, ...]\n",
    "    return image\n",
    "  else:\n",
    "    with open(output_file, 'wb') as f:\n",
    "      im_fmt = 'png' if output_file.endswith('.png') else 'pdf'\n",
    "      data_utils.plot_samples(float_data, f, im_size=4.1, im_fmt=im_fmt,axis=True)\n",
    "\n",
    "\n",
    "class BinarySyntheticHelper(object):\n",
    "  \"\"\"Binary synthetic model helper.\"\"\"\n",
    "\n",
    "  def __init__(self, seq_len,int_scale):\n",
    "    self.seq_len = seq_len\n",
    "    self.int_scale=int_scale\n",
    "    self.bm, self.inv_bm = data_utils.get_binmap(seq_len,\n",
    "                                                 'gray')\n",
    "\n",
    "  def plot(self, xbin, output_file=None):\n",
    "    fn_xbin2float = functools.partial(\n",
    "        data_utils.bin2float, inv_bm=self.inv_bm,\n",
    "        discrete_dim=self.seq_len, int_scale=self.int_scale)\n",
    "    return plot(xbin, fn_xbin2float, output_file)\n",
    "\n",
    "int_scale=5461.760975376213\n",
    "model_helper = BinarySyntheticHelper(seq_len,int_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metric used in Table 1 of Lou2023\n",
    "def binary_mmd(x, y, sim_fn):\n",
    "  \"\"\"MMD for binary data.\"\"\"\n",
    "  x = x.astype(np.float32)\n",
    "  y = y.astype(np.float32)\n",
    "  kxx = sim_fn(x, x)\n",
    "  kxx = kxx * (1 - np.eye(x.shape[0]))\n",
    "  kxx = np.sum(kxx) / x.shape[0] / (x.shape[0] - 1)\n",
    "\n",
    "  kyy = sim_fn(y, y)\n",
    "  kyy = kyy * (1 - np.eye(y.shape[0]))\n",
    "  kyy = np.sum(kyy) / y.shape[0] / (y.shape[0] - 1)\n",
    "  kxy = np.sum(sim_fn(x, y))\n",
    "  kxy = kxy / x.shape[0] / y.shape[0]\n",
    "  mmd = kxx + kyy - 2 * kxy\n",
    "  return mmd\n",
    "\n",
    "def binary_exp_hamming_sim(x, y, bd):\n",
    "  x = np.expand_dims(x, axis=1)\n",
    "  y = np.expand_dims(y, axis=0)\n",
    "  d = np.sum(np.abs(x - y), axis=-1)\n",
    "  return np.exp(-bd * d)\n",
    "\n",
    "def binary_exp_hamming_mmd(x, y, bandwidth=0.1):\n",
    "  sim_fn = functools.partial(binary_exp_hamming_sim, bd=bandwidth)\n",
    "  return binary_mmd(x, y, sim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at step 0.\n"
     ]
    }
   ],
   "source": [
    "num_train_steps=cfg.training.n_iters\n",
    "log_freq=cfg.training.log_freq\n",
    "snapshot_freq=cfg.training.snapshot_freq\n",
    "eval_rounds=1\n",
    "plot_samples=cfg.plot_samples\n",
    "# Build one-step training and evaluation functions\n",
    "optimize_fn = losses.optimization_manager(cfg)\n",
    "train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "num_train_steps = cfg.training.n_iters\n",
    "initial_step = int(state['step'])\n",
    "print(f\"Starting training loop at step {initial_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the metric used in Table1 of Lou2023. Should get at least as small as 1.62e-5 to be considered done training. \n",
    "def run_sampling(score_model,save_step):\n",
    "    avg_mmd = 0.0\n",
    "    for i in range(eval_rounds):\n",
    "        gt_data = []\n",
    "        for _ in range(plot_samples // batch_size):\n",
    "            gt_data.append(next(train_ds).cpu().numpy())\n",
    "        gt_data = np.concatenate(gt_data, axis=0)\n",
    "        gt_data = np.reshape(gt_data, (-1, seq_len))\n",
    "        sample_data=[]\n",
    "        for _ in range(plot_samples // batch_size):\n",
    "            sample_data.append(sampling_fn(score_model).cpu().numpy())\n",
    "        sample_data=np.concatenate(sample_data,axis=0)\n",
    "        x0 = np.reshape(sample_data, gt_data.shape)\n",
    "        mmd = binary_exp_hamming_mmd(x0, gt_data)\n",
    "        avg_mmd += mmd\n",
    "        model_helper.plot(x0,os.path.join(sample_dir,f'step_{save_step}_eval_round_{i}.pdf')) #plot a sample from the model\n",
    "    avg_mmd = avg_mmd / eval_rounds\n",
    "    return avg_mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only run this if you want to load in an old checkpoint. For some reason state = utils.restore_checkpoint is not working\n",
    "loaded_state = torch.load(checkpoint_dir+'//checkpoint_30.pth', map_location=device,weights_only=False)\n",
    "score_model.load_state_dict(loaded_state['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.00029790387683237274)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how you can run sampling without retraining\n",
    "#for some reason the analytic predictor performs very poorly. \n",
    "# My guess is this is because they do not enforce the score to be 1 on the diagonals\n",
    "#In particular, the score for masked positions is actually being used for the probability of a token remaining masked, despite this not being trained anywhere\n",
    "#For Euler this doesn't matter, since to construct the new rate matrix with graph.reverse_rate, the diagonals are zeroed out and replaced by the - column sums\n",
    "#It could also just be a bug\n",
    "cfg.sampling.predictor='euler'\n",
    "sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "run_sampling(score_model,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training_loss: 1.41226e+01\n",
      "step: 1000, training_loss: 7.29549e+00\n",
      "step: 2000, training_loss: 6.39916e+00\n",
      "step: 3000, training_loss: 7.38684e+00\n",
      "step: 4000, training_loss: 8.18592e+00\n",
      "step: 5000, training_loss: 4.82452e+01\n",
      "step: 6000, training_loss: 5.81248e+00\n",
      "step: 7000, training_loss: 8.41568e+00\n",
      "step: 8000, training_loss: 7.31727e+00\n",
      "step: 9000, training_loss: 6.00721e+00\n",
      "step: 10000, training_loss: 7.49004e+00\n",
      "Average mmd : 0.0003640006553522479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11000, training_loss: 7.20117e+00\n",
      "step: 12000, training_loss: 6.31513e+00\n",
      "step: 13000, training_loss: 1.00243e+01\n",
      "step: 14000, training_loss: 6.22494e+00\n",
      "step: 15000, training_loss: 1.50774e+01\n",
      "step: 16000, training_loss: 7.63878e+00\n",
      "step: 17000, training_loss: 5.47862e+00\n",
      "step: 18000, training_loss: 4.78853e+00\n",
      "step: 19000, training_loss: 8.57851e+00\n",
      "step: 20000, training_loss: 8.88760e+00\n",
      "Average mmd : 0.0005333319484661647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 21000, training_loss: 8.54827e+00\n",
      "step: 22000, training_loss: 4.84739e+00\n",
      "step: 23000, training_loss: 7.53565e+00\n",
      "step: 24000, training_loss: 6.68250e+00\n",
      "step: 25000, training_loss: 5.91620e+00\n",
      "step: 26000, training_loss: 6.97186e+00\n",
      "step: 27000, training_loss: 7.15542e+00\n",
      "step: 28000, training_loss: 5.55506e+00\n",
      "step: 29000, training_loss: 6.30056e+00\n",
      "step: 30000, training_loss: 7.87461e+00\n",
      "Average mmd : 0.00028390120539179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31000, training_loss: 6.29522e+00\n",
      "step: 32000, training_loss: 6.47704e+00\n",
      "step: 33000, training_loss: 6.49403e+00\n",
      "step: 34000, training_loss: 8.06421e+00\n",
      "step: 35000, training_loss: 8.34981e+00\n",
      "step: 36000, training_loss: 7.02493e+00\n",
      "step: 37000, training_loss: 5.69342e+00\n",
      "step: 38000, training_loss: 6.88028e+00\n",
      "step: 39000, training_loss: 5.99629e+00\n",
      "step: 40000, training_loss: 6.28630e+00\n",
      "Average mmd : 0.00012038011993886766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41000, training_loss: 7.84677e+00\n",
      "step: 42000, training_loss: 7.75513e+00\n",
      "step: 43000, training_loss: 6.73912e+00\n",
      "step: 44000, training_loss: 6.69367e+00\n",
      "step: 45000, training_loss: 5.69774e+00\n",
      "step: 46000, training_loss: 5.77730e+00\n",
      "step: 47000, training_loss: 6.53800e+00\n",
      "step: 48000, training_loss: 5.39389e+00\n",
      "step: 49000, training_loss: 5.71389e+00\n",
      "step: 50000, training_loss: 6.37098e+00\n",
      "Average mmd : 5.086176924623542e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51000, training_loss: 1.00758e+01\n",
      "step: 52000, training_loss: 5.56583e+00\n",
      "step: 53000, training_loss: 5.88243e+00\n",
      "step: 54000, training_loss: 6.86145e+00\n",
      "step: 55000, training_loss: 7.03091e+00\n",
      "step: 56000, training_loss: 5.79215e+00\n",
      "step: 57000, training_loss: 8.24274e+00\n",
      "step: 58000, training_loss: 6.88065e+00\n",
      "step: 59000, training_loss: 6.06866e+00\n",
      "step: 60000, training_loss: 6.68651e+00\n",
      "Average mmd : 0.0001695241510300538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61000, training_loss: 6.34751e+00\n",
      "step: 62000, training_loss: 6.95545e+00\n",
      "step: 63000, training_loss: 6.79104e+00\n",
      "step: 64000, training_loss: 6.64213e+00\n",
      "step: 65000, training_loss: 9.87277e+00\n",
      "step: 66000, training_loss: 6.00274e+00\n",
      "step: 67000, training_loss: 5.19443e+00\n",
      "step: 68000, training_loss: 6.19763e+00\n",
      "step: 69000, training_loss: 7.54788e+00\n",
      "step: 70000, training_loss: 6.44177e+00\n",
      "Average mmd : 4.863812519101396e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 71000, training_loss: 5.83654e+00\n",
      "step: 72000, training_loss: 1.05447e+01\n",
      "step: 73000, training_loss: 9.54310e+00\n",
      "step: 74000, training_loss: 6.95478e+00\n",
      "step: 75000, training_loss: 9.93359e+00\n",
      "step: 76000, training_loss: 5.30638e+00\n",
      "step: 77000, training_loss: 5.32468e+00\n",
      "step: 78000, training_loss: 1.69249e+01\n",
      "step: 79000, training_loss: 7.35572e+00\n",
      "step: 80000, training_loss: 6.98753e+00\n",
      "Average mmd : 0.00013383285598972394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 81000, training_loss: 5.80521e+00\n",
      "step: 82000, training_loss: 6.34355e+00\n",
      "step: 83000, training_loss: 6.75836e+00\n",
      "step: 84000, training_loss: 8.75452e+00\n",
      "step: 85000, training_loss: 7.13454e+00\n",
      "step: 86000, training_loss: 7.09371e+00\n",
      "step: 87000, training_loss: 5.73604e+00\n",
      "step: 88000, training_loss: 5.89546e+00\n",
      "step: 89000, training_loss: 6.71797e+00\n",
      "step: 90000, training_loss: 5.25486e+00\n",
      "Average mmd : 0.00016679028102728477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 91000, training_loss: 8.05806e+00\n",
      "step: 92000, training_loss: 6.16469e+00\n",
      "step: 93000, training_loss: 4.93728e+00\n",
      "step: 94000, training_loss: 5.14612e+00\n",
      "step: 95000, training_loss: 6.94902e+00\n",
      "step: 96000, training_loss: 5.17130e+00\n",
      "step: 97000, training_loss: 4.47778e+00\n",
      "step: 98000, training_loss: 5.50824e+00\n",
      "step: 99000, training_loss: 8.44211e+00\n",
      "step: 100000, training_loss: 7.55701e+00\n",
      "Average mmd : 0.0002918816018480097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 101000, training_loss: 2.66682e+01\n",
      "step: 102000, training_loss: 6.34122e+00\n",
      "step: 103000, training_loss: 6.73332e+00\n",
      "step: 104000, training_loss: 9.63464e+00\n",
      "step: 105000, training_loss: 7.59741e+00\n",
      "step: 106000, training_loss: 5.06771e+00\n",
      "step: 107000, training_loss: 9.49790e+00\n",
      "step: 108000, training_loss: 4.48823e+00\n",
      "step: 109000, training_loss: 7.96648e+00\n",
      "step: 110000, training_loss: 6.24460e+00\n",
      "Average mmd : 9.020412417315438e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 111000, training_loss: 8.35492e+00\n",
      "step: 112000, training_loss: 6.79260e+00\n",
      "step: 113000, training_loss: 7.06547e+00\n",
      "step: 114000, training_loss: 7.88346e+00\n",
      "step: 115000, training_loss: 7.61630e+00\n",
      "step: 116000, training_loss: 7.34974e+00\n",
      "step: 117000, training_loss: 5.91417e+00\n",
      "step: 118000, training_loss: 6.69014e+00\n",
      "step: 119000, training_loss: 8.05774e+00\n",
      "step: 120000, training_loss: 6.62320e+00\n",
      "Average mmd : 8.313656158831506e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 121000, training_loss: 6.54395e+00\n",
      "step: 122000, training_loss: 7.08533e+00\n",
      "step: 123000, training_loss: 9.04494e+00\n",
      "step: 124000, training_loss: 5.07058e+00\n",
      "step: 125000, training_loss: 7.15364e+00\n",
      "step: 126000, training_loss: 6.44002e+00\n",
      "step: 127000, training_loss: 6.01305e+00\n",
      "step: 128000, training_loss: 6.61388e+00\n",
      "step: 129000, training_loss: 7.58973e+00\n",
      "step: 130000, training_loss: 5.86444e+00\n",
      "Average mmd : 0.0002010154093174954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 131000, training_loss: 5.97580e+00\n",
      "step: 132000, training_loss: 6.62909e+00\n",
      "step: 133000, training_loss: 4.49641e+00\n",
      "step: 134000, training_loss: 6.00001e+00\n",
      "step: 135000, training_loss: 8.74806e+00\n",
      "step: 136000, training_loss: 5.46258e+00\n",
      "step: 137000, training_loss: 8.31015e+00\n",
      "step: 138000, training_loss: 7.40444e+00\n",
      "step: 139000, training_loss: 7.24390e+00\n",
      "step: 140000, training_loss: 6.50993e+00\n",
      "Average mmd : 0.00017007240940619672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 141000, training_loss: 5.46711e+00\n",
      "step: 142000, training_loss: 6.47495e+00\n",
      "step: 143000, training_loss: 5.13338e+00\n",
      "step: 144000, training_loss: 6.69692e+00\n",
      "step: 145000, training_loss: 5.06469e+00\n",
      "step: 146000, training_loss: 5.24880e+00\n",
      "step: 147000, training_loss: 7.59048e+00\n",
      "step: 148000, training_loss: 6.75145e+00\n",
      "step: 149000, training_loss: 4.79330e+00\n",
      "step: 150000, training_loss: 1.01656e+01\n",
      "Average mmd : 0.00016242291064316738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 151000, training_loss: 5.70037e+00\n",
      "step: 152000, training_loss: 8.53155e+00\n",
      "step: 153000, training_loss: 8.28711e+00\n",
      "step: 154000, training_loss: 5.67736e+00\n",
      "step: 155000, training_loss: 5.38800e+00\n",
      "step: 156000, training_loss: 8.95873e+00\n",
      "step: 157000, training_loss: 4.59274e+00\n",
      "step: 158000, training_loss: 6.25256e+00\n",
      "step: 159000, training_loss: 5.34112e+00\n",
      "step: 160000, training_loss: 7.83740e+00\n",
      "Average mmd : 9.468582584137852e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 161000, training_loss: 6.26759e+00\n",
      "step: 162000, training_loss: 7.25783e+00\n",
      "step: 163000, training_loss: 5.74947e+00\n",
      "step: 164000, training_loss: 6.92614e+00\n",
      "step: 165000, training_loss: 1.00917e+01\n",
      "step: 166000, training_loss: 6.43917e+00\n",
      "step: 167000, training_loss: 5.83975e+00\n",
      "step: 168000, training_loss: 6.11093e+00\n",
      "step: 169000, training_loss: 6.53162e+00\n",
      "step: 170000, training_loss: 5.89162e+00\n",
      "Average mmd : 0.0003490734321175415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 171000, training_loss: 6.35228e+00\n",
      "step: 172000, training_loss: 1.13059e+01\n",
      "step: 173000, training_loss: 6.58251e+00\n",
      "step: 174000, training_loss: 8.23549e+00\n",
      "step: 175000, training_loss: 1.12891e+01\n",
      "step: 176000, training_loss: 6.66461e+00\n",
      "step: 177000, training_loss: 7.13236e+00\n",
      "step: 178000, training_loss: 7.73617e+00\n",
      "step: 179000, training_loss: 8.67772e+00\n",
      "step: 180000, training_loss: 7.06429e+00\n",
      "Average mmd : 0.00019060092025391384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 181000, training_loss: 5.70394e+00\n",
      "step: 182000, training_loss: 1.05196e+01\n",
      "step: 183000, training_loss: 5.58397e+00\n",
      "step: 184000, training_loss: 5.73516e+00\n",
      "step: 185000, training_loss: 8.83622e+00\n",
      "step: 186000, training_loss: 7.82314e+00\n",
      "step: 187000, training_loss: 6.06141e+00\n",
      "step: 188000, training_loss: 8.21182e+00\n",
      "step: 189000, training_loss: 5.86241e+00\n",
      "step: 190000, training_loss: 6.38748e+00\n",
      "Average mmd : 0.0002595504474547372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 191000, training_loss: 7.75131e+00\n",
      "step: 192000, training_loss: 7.23712e+00\n",
      "step: 193000, training_loss: 5.83502e+00\n",
      "step: 194000, training_loss: 6.97503e+00\n",
      "step: 195000, training_loss: 6.01744e+00\n",
      "step: 196000, training_loss: 7.66085e+00\n",
      "step: 197000, training_loss: 5.46033e+00\n",
      "step: 198000, training_loss: 7.12464e+00\n",
      "step: 199000, training_loss: 5.59343e+00\n",
      "step: 200000, training_loss: 9.05375e+00\n",
      "Average mmd : 0.00018088776639901827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 201000, training_loss: 6.47947e+00\n",
      "step: 202000, training_loss: 5.72223e+00\n",
      "step: 203000, training_loss: 6.47357e+00\n",
      "step: 204000, training_loss: 6.68989e+00\n",
      "step: 205000, training_loss: 8.49762e+00\n",
      "step: 206000, training_loss: 6.37124e+00\n",
      "step: 207000, training_loss: 6.67030e+00\n",
      "step: 208000, training_loss: 5.81955e+00\n",
      "step: 209000, training_loss: 9.32071e+00\n",
      "step: 210000, training_loss: 7.21467e+00\n",
      "Average mmd : 0.00017813098686120243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 211000, training_loss: 5.37753e+00\n",
      "step: 212000, training_loss: 8.11352e+00\n",
      "step: 213000, training_loss: 6.80517e+00\n",
      "step: 214000, training_loss: 6.22781e+00\n",
      "step: 215000, training_loss: 7.32033e+00\n",
      "step: 216000, training_loss: 7.18331e+00\n",
      "step: 217000, training_loss: 8.14957e+00\n",
      "step: 218000, training_loss: 6.30968e+00\n",
      "step: 219000, training_loss: 7.92091e+00\n",
      "step: 220000, training_loss: 7.40144e+00\n",
      "Average mmd : 0.00019098800607364463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 221000, training_loss: 5.76765e+00\n",
      "step: 222000, training_loss: 9.33074e+00\n",
      "step: 223000, training_loss: 7.93380e+00\n",
      "step: 224000, training_loss: 1.01962e+01\n",
      "step: 225000, training_loss: 6.40387e+00\n",
      "step: 226000, training_loss: 7.81309e+00\n",
      "step: 227000, training_loss: 9.11794e+00\n",
      "step: 228000, training_loss: 6.34831e+00\n",
      "step: 229000, training_loss: 5.87277e+00\n",
      "step: 230000, training_loss: 6.55686e+00\n",
      "Average mmd : 0.00030725405402853845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 231000, training_loss: 6.27106e+00\n",
      "step: 232000, training_loss: 6.70316e+00\n",
      "step: 233000, training_loss: 6.16224e+00\n",
      "step: 234000, training_loss: 6.43222e+00\n",
      "step: 235000, training_loss: 6.66565e+00\n",
      "step: 236000, training_loss: 9.01056e+00\n",
      "step: 237000, training_loss: 7.26311e+00\n",
      "step: 238000, training_loss: 5.60443e+00\n",
      "step: 239000, training_loss: 6.26367e+00\n",
      "step: 240000, training_loss: 7.83426e+00\n",
      "Average mmd : 0.00029431629016080096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 241000, training_loss: 7.08268e+00\n",
      "step: 242000, training_loss: 5.80816e+00\n",
      "step: 243000, training_loss: 6.77672e+00\n",
      "step: 244000, training_loss: 7.64548e+00\n",
      "step: 245000, training_loss: 7.52814e+00\n",
      "step: 246000, training_loss: 5.95119e+00\n",
      "step: 247000, training_loss: 6.63526e+00\n",
      "step: 248000, training_loss: 6.34319e+00\n",
      "step: 249000, training_loss: 6.13735e+00\n",
      "step: 250000, training_loss: 4.44191e+00\n",
      "Average mmd : 0.00033410929097421604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 251000, training_loss: 7.32456e+00\n",
      "step: 252000, training_loss: 6.08945e+00\n",
      "step: 253000, training_loss: 5.41248e+00\n",
      "step: 254000, training_loss: 8.48174e+00\n",
      "step: 255000, training_loss: 7.77450e+00\n",
      "step: 256000, training_loss: 5.43819e+00\n",
      "step: 257000, training_loss: 6.01581e+00\n",
      "step: 258000, training_loss: 8.52086e+00\n",
      "step: 259000, training_loss: 7.61097e+00\n",
      "step: 260000, training_loss: 1.79977e+01\n",
      "Average mmd : 0.00039947567637660386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 261000, training_loss: 8.50192e+00\n",
      "step: 262000, training_loss: 7.22020e+00\n",
      "step: 263000, training_loss: 4.27770e+00\n",
      "step: 264000, training_loss: 6.75724e+00\n",
      "step: 265000, training_loss: 7.62675e+00\n",
      "step: 266000, training_loss: 5.79714e+00\n",
      "step: 267000, training_loss: 7.44918e+00\n",
      "step: 268000, training_loss: 6.82013e+00\n",
      "step: 269000, training_loss: 5.60125e+00\n",
      "step: 270000, training_loss: 9.31445e+00\n",
      "Average mmd : 0.00039709591426828617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 271000, training_loss: 5.65927e+00\n",
      "step: 272000, training_loss: 5.96117e+00\n",
      "step: 273000, training_loss: 6.95887e+00\n",
      "step: 274000, training_loss: 5.81115e+00\n",
      "step: 275000, training_loss: 5.64223e+00\n",
      "step: 276000, training_loss: 6.00206e+00\n",
      "step: 277000, training_loss: 6.05838e+00\n",
      "step: 278000, training_loss: 6.14898e+00\n",
      "step: 279000, training_loss: 7.19445e+00\n",
      "step: 280000, training_loss: 5.00060e+00\n",
      "Average mmd : 0.00043629542371792507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 281000, training_loss: 5.63261e+00\n",
      "step: 282000, training_loss: 6.73427e+00\n",
      "step: 283000, training_loss: 6.84433e+00\n",
      "step: 284000, training_loss: 6.59641e+00\n",
      "step: 285000, training_loss: 7.01850e+00\n",
      "step: 286000, training_loss: 6.07613e+00\n",
      "step: 287000, training_loss: 6.06228e+00\n",
      "step: 288000, training_loss: 6.40427e+00\n",
      "step: 289000, training_loss: 8.07224e+00\n",
      "step: 290000, training_loss: 7.13761e+00\n",
      "Average mmd : 0.00027940588081754036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bezem\\Documents\\REU code\\model\\utils.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 291000, training_loss: 6.44657e+00\n",
      "step: 292000, training_loss: 6.27870e+00\n",
      "step: 293000, training_loss: 5.07977e+00\n",
      "step: 294000, training_loss: 1.24528e+01\n",
      "step: 295000, training_loss: 5.29632e+00\n",
      "step: 296000, training_loss: 7.60272e+00\n",
      "step: 297000, training_loss: 5.56409e+00\n",
      "step: 298000, training_loss: 9.05021e+00\n",
      "step: 299000, training_loss: 7.01238e+00\n",
      "step: 300000, training_loss: 6.50277e+00\n",
      "Average mmd : 0.0003802995859849556\n"
     ]
    }
   ],
   "source": [
    "for step in range(initial_step,num_train_steps+1):\n",
    "#while state['step'] < num_train_steps + 1:\n",
    "    #step = state['step']\n",
    "    batch=next(train_iter).to(device)\n",
    "    # Compute loss using the simplified loss function\n",
    "    loss=train_step_fn(state, batch)\n",
    "    \n",
    "        \n",
    "    if step % log_freq == 0:\n",
    "        print(\"step: %d, training_loss: %.5e\" % (step, loss.mean().item()))\n",
    "            \n",
    "    if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "        # Save the checkpoint.\n",
    "        save_step = step // cfg.training.snapshot_freq\n",
    "        utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "        \n",
    "        #want to use the ema weights for sampling\n",
    "        ema.store(score_model.parameters())\n",
    "        ema.copy_to(score_model.parameters())\n",
    "\n",
    "        avg_mmd=run_sampling(score_model,save_step)\n",
    "        \n",
    "        with open('output.txt', 'a') as file:\n",
    "            file.write(f'time: {datetime.now()},step: {save_step}, loss: {loss.mean().item()}, MMD: {avg_mmd}\\n')\n",
    "            \n",
    "        ema.restore(score_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
